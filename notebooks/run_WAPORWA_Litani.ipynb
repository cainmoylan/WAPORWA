{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READ-ME\n",
    "\n",
    "This ipython notebook is used to run WaPOR-based Rapid WA+ analysis. \n",
    "\n",
    "## Flow-chart\n",
    "\n",
    "## Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Start calculation\n",
    "**Edit BASIN_NAME. Correct paths: MAIN_DIR, INPUT_DIR, PCKG_DIR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Jul 23 12:36:04 2019\n",
    "\n",
    "@author: ntr002\n",
    "\"\"\"\n",
    "import os\n",
    "WORKING_DIR=r\"D:\\FAO\\WA_Sheet1\"\n",
    "MAIN_DIR=os.path.join(WORKING_DIR,\"Main\")\n",
    "INPUT_DIR=os.path.join(WORKING_DIR,\"Input\")\n",
    "PCKG_DIR=os.path.join(WORKING_DIR,\"WAPORWA\")\n",
    "### Import local package\n",
    "\n",
    "os.chdir(PCKG_DIR) \n",
    "import WaPOR\n",
    "import WA\n",
    "from WA.pickle_basin import pickle_in, pickle_out\n",
    "### Basin\n",
    "BASIN_NAME='Litani'\n",
    "INPUT_FOLDER=os.path.join(INPUT_DIR,BASIN_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global\n",
    "GRaND_SHP=r\"D:\\FAO\\WA_Sheet1\\Input\\Global\\GRanD\\GRanD_reservoirs_v1_1_clipped.shp\"\n",
    "WDPA_SHP=r\"D:\\FAO\\WA_Sheet1\\Input\\Global\\WDPA\\WDPA_CatIandII_17countries.shp\"\n",
    "ThetaSat_TIF=r\"D:\\FAO\\WA_Sheet1\\Input\\Global\\HiHydroSoils\\thetasat_topsoil.tif\"\n",
    "\n",
    "#Basin\n",
    "BASIN_SHP=r\"D:\\FAO\\WA_Sheet1\\Input\\Litani\\Geo\\Shapefile\\Litani.shp\"\n",
    "GRACEMonthly_CSV=r\"D:\\FAO\\Litani\\WA_Data\\GRACE\\Litani_GRACE.csv\"\n",
    "unreserv_SHP=None\n",
    "makereserv_SHP=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1. Open latest pickle (saved metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\FAO\\WA_Sheet1\\Main\\Litani\\Info_20191106_11h33.pickle\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "latest_pickle=glob.glob(os.path.join(MAIN_DIR,BASIN_NAME,'*.pickle'))[-1]\n",
    "print(latest_pickle)\n",
    "BASIN=pickle_in(latest_pickle)\n",
    "end_month=BASIN['end_month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Download WaPOR data\n",
    "**Edit xmin, ymin, xmax, ymax**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading WaPOR catalog...\n",
      "\n",
      "Download monthly WaPOR Reference Evapotranspiration data for the period 2009-01-01 till 2019-01-01\n",
      "Loading WaPOR catalog...\n",
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete\n"
     ]
    }
   ],
   "source": [
    "WaPOR.API.version=2\n",
    "catalog=WaPOR.API.getCatalog()\n",
    "xmin,ymin,xmax,ymax=[35.22916666666592, 33.0999999999998, 36.399999999999224, 34.04999999999972]\n",
    "\n",
    "# WaPOR.PCP_daily(INPUT_FOLDER,Startdate='2009-01-01',\n",
    "#                   Enddate='2019-01-01',latlim=[ymin-0.25,ymax+0.25],\n",
    "#                   lonlim=[xmin-0.25,xmax+0.25])\n",
    "\n",
    "# WaPOR.PCP_monthly(INPUT_FOLDER,Startdate='2009-01-01',\n",
    "#                   Enddate='2019-01-01',latlim=[ymin-0.25,ymax+0.25],\n",
    "#                   lonlim=[xmin-0.25,xmax+0.25])\n",
    "\n",
    "WaPOR.RET_monthly(INPUT_FOLDER,Startdate='2009-01-01',\n",
    "                  Enddate='2019-01-01',latlim=[ymin-0.5,ymax+0.5],\n",
    "                  lonlim=[xmin-0.5,xmax+0.5])\n",
    "\n",
    "# WaPOR.AET_monthly(INPUT_FOLDER,Startdate='2009-01-01',\n",
    "#                   Enddate='2019-01-01',latlim=[ymin,ymax],\n",
    "#                   lonlim=[xmin,xmax],level=2)\n",
    "\n",
    "# WaPOR.LCC_yearly(INPUT_FOLDER,Startdate='2009-01-01',\n",
    "#                   Enddate='2019-01-01',latlim=[ymin,ymax],\n",
    "#                   lonlim=[xmin,xmax],level=2)\n",
    "\n",
    "# WaPOR.I_dekadal(INPUT_FOLDER,Startdate='2009-01-01',\n",
    "#                   Enddate='2019-01-01',latlim=[ymin,ymax],\n",
    "#                   lonlim=[xmin,xmax],level=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Initilize Basin metadata dictionary\n",
    "**Edit: Basin metadata**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIN={'Name':BASIN_NAME,\n",
    "       'time_range':['2009-01-01','2018-12-31'],\n",
    "       'end_month':'AUG',\n",
    "       'Dir':os.path.join(MAIN_DIR,BASIN_NAME),       \n",
    "        'geo_data':{\n",
    "                   'basin':BASIN_SHP,\n",
    "                   'unreserv':unreserv_SHP, #If GRaND reservoir map need to be adjusted\n",
    "                   'makereserv':makereserv_SHP,#If GRaND reservoir map need to be adjusted\n",
    "                      },\n",
    "        'global_data':{                     \n",
    "                     'grand':GRaND_SHP,\n",
    "                     'wdpa':WDPA_SHP,\n",
    "                     },\n",
    "        'input_data':{\n",
    "                      'yearly':{\n",
    "                              'lcc':[r\"\"+os.path.join(INPUT_FOLDER,'L2_LCC_A'),\n",
    "                                     '-','Landcover Class'],\n",
    "                              },\n",
    "                      'monthly':{\n",
    "                              'p':[r\"\"+os.path.join(INPUT_FOLDER,'L1_PCP_M'),\n",
    "                                   'mm/month','Precipitation'],\n",
    "                              'et':[r\"\"+os.path.join(INPUT_FOLDER,'L2_AETI_M'),\n",
    "                                    'mm/month','Actual Evapotranspiration'],\n",
    "                              'ret':[r\"\"+os.path.join(INPUT_FOLDER,'L1_RET_M'),\n",
    "                                    'mm/month','Reference Evapotranspiration']                                    \n",
    "                              },  \n",
    "                    'stat':{                                                          \n",
    "                            'area':None,                 \n",
    "                            'thetasat':ThetaSat_TIF\n",
    "                            }                                                   \n",
    "                              },\n",
    "        'input_ts':{\n",
    "                'dS':GRACEMonthly_CSV,\n",
    "                \n",
    "                    },\n",
    "        'main_data':{\n",
    "                'yearly':{},\n",
    "                     'monthly':{},\n",
    "                     'stat':{},\n",
    "                     }                \n",
    "                }\n",
    "\n",
    "if not os.path.exists(BASIN['Dir']):\n",
    "    os.makedirs(BASIN['Dir'])\n",
    "pickle_out(BASIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Compute intermediate data\n",
    "## 3.1. Calculate monthly Interception from L2_I_D\n",
    "**If L2_I_M is not available via API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Sep 11 16:42:09 2019\n",
    "\n",
    "@author: ntr002\n",
    "\"\"\"\n",
    "import os\n",
    "os.chdir(PCKG_DIR)\n",
    "import glob\n",
    "import pandas as pd\n",
    "from osgeo import gdal\n",
    "import numpy as np\n",
    "import osr\n",
    "from WaPOR import GIS_functions as gis\n",
    "\n",
    "#%%\n",
    "#get list of dekadal rasters\n",
    "Dekadal_I_folder=os.path.join(INPUT_FOLDER,'L2_I_D')\n",
    "input_fhs=glob.glob(os.path.join(Dekadal_I_folder,'*.tif'))\n",
    "input_fhs\n",
    "\n",
    "#Get month dates\n",
    "start_date=BASIN['time_range'][0]\n",
    "end_date=BASIN['time_range'][1]\n",
    "month_dates=pd.date_range(start_date,end_date,freq='M')\n",
    "\n",
    "#Get df avail \n",
    "ds_code='L2_I_D' \n",
    "WaPOR.API.version=2\n",
    "df_avail=WaPOR.API.getAvailData(ds_code,time_range='{0},{1}'.format(start_date,end_date))\n",
    "\n",
    "#aggregate monthly\n",
    "output_folder=os.path.join(INPUT_FOLDER,'L2_I_M')\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "driver, NDV, xsize, ysize, GeoT, Projection = gis.GetGeoInfo(input_fhs[0])\n",
    "for date in month_dates:\n",
    "    month_fhs=[]\n",
    "    for in_fh in input_fhs:\n",
    "        raster_id=os.path.split(in_fh)[-1].split('.tif')[0][-9:]\n",
    "        raster_info=df_avail.loc[df_avail['raster_id']==raster_id]\n",
    "        timestr=raster_info['DEKAD'].iloc[0]\n",
    "        year=int(timestr[0:4])\n",
    "        month=int(timestr[5:7])\n",
    "        if (year==date.year)&(month==date.month):\n",
    "            month_fhs.append(in_fh)\n",
    "    SumArray=np.zeros((ysize,xsize),dtype=np.float32)\n",
    "    for fh in month_fhs:\n",
    "        Array=gis.OpenAsArray(fh,nan_values=True)\n",
    "        SumArray+=Array\n",
    "    out_fh=os.path.join(output_folder,'L2_I_{:04d}{:02d}.tif'.format(date.year,date.month))    \n",
    "    gis.CreateGeoTiff(out_fh, SumArray, driver, NDV, xsize, ysize, GeoT, Projection)\n",
    "BASIN['input_data']['monthly']['i']=[r\"\"+os.path.join(INPUT_FOLDER,'L2_I_M'),'mm/month','Interception']\n",
    "pickle_out(BASIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Calculate number of rainy days per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adapted from watools (by T.Hessels)\n",
    "\"\"\"\n",
    "import calendar\n",
    "import glob\n",
    "import os\n",
    "from WaPOR import GIS_functions as gis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "###\n",
    "Data_Path_P=os.path.join(INPUT_FOLDER,'L1_PCP_E')\n",
    "Startdate=BASIN['time_range'][0]\n",
    "Enddate=BASIN['time_range'][1]\n",
    "\n",
    "Data_Path_RD = os.path.join(BASIN['Dir'],'data', 'Rainy_Days')\n",
    "if not os.path.exists(Data_Path_RD):\n",
    "    os.mkdir(Data_Path_RD)\n",
    "Dates = pd.date_range(Startdate, Enddate, freq ='MS')\n",
    "os.chdir(Data_Path_P)\n",
    "\n",
    "for Date in Dates:\n",
    "    # Define the year and month and amount of days in month\n",
    "    year = Date.year\n",
    "    month = Date.month\n",
    "    daysinmonth = calendar.monthrange(year, month)[1]\n",
    "\n",
    "    # Set the third (time) dimension of array starting at 0\n",
    "    i = 0\n",
    "\n",
    "    # Find all files of that month\n",
    "    files = glob.glob('*daily_%d.%02d.*.tif' %(year, month))\n",
    "\n",
    "    # Check if the amount of files corresponds with the amount of days in month\n",
    "    if len(files) is not daysinmonth:\n",
    "        print('ERROR: Not all Rainfall days for month %d and year %d are downloaded'  %(month, year))\n",
    "\n",
    "    # Loop over the days and store data in raster\n",
    "    for File in files:\n",
    "        dir_file = os.path.join(Data_Path_P, File)\n",
    "\n",
    "        # Get array information and create empty numpy array for daily rainfall when looping the first file\n",
    "        if File == files[0]:\n",
    "\n",
    "            # Open geolocation info and define projection\n",
    "            driver, NDV, xsize, ysize, GeoT, Projection = gis.GetGeoInfo(dir_file)\n",
    "\n",
    "            # Create empty array for the whole month\n",
    "            P_Daily = np.zeros([daysinmonth,ysize, xsize])\n",
    "\n",
    "        # Open data and put the data in 3D array\n",
    "        Data = gis.OpenAsArray(dir_file,nan_values=True)\n",
    "\n",
    "        # Remove the weird numbers\n",
    "        Data[Data<0] = 0\n",
    "\n",
    "        # Add the precipitation to the monthly cube\n",
    "        P_Daily[i, :, :] = Data\n",
    "        i += 1\n",
    "\n",
    "    # Define a rainy day\n",
    "    P_Daily[P_Daily > 0.201] = 1\n",
    "    P_Daily[P_Daily != 1] = 0\n",
    "\n",
    "    # Sum the amount of rainy days\n",
    "    RD_one_month = np.nansum(P_Daily,0)\n",
    "\n",
    "    # Define output name\n",
    "    Outname = os.path.join(Data_Path_RD, 'Rainy_Days_NumOfDays_monthly_%d.%02d.01.tif' %(year, month))\n",
    "\n",
    "    # Save tiff file\n",
    "    gis.CreateGeoTiff(Outname, RD_one_month, driver, NDV, xsize, ysize, GeoT, Projection)\n",
    "    \n",
    "BASIN['input_data']['monthly']['nRD']=[Data_Path_RD,'days/month','Number of rainy days']\n",
    "pickle_out(BASIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Reclassify WaPOR LCC to LUWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-0b8f982999a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m### Get all yearly WaPOR LCC maps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mLCC_fhs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBASIN\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'input_data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'yearly'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lcc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'*.tif'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mWaPOR_LCC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLCC_fhs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#template\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Created on Thu Aug  8 13:42:44 2019\n",
    "\n",
    "@author: ntr002\n",
    "WAPOR LCC to LUWA\n",
    "\n",
    "Note: start Jupyter notebook with \"conda activate env\"\n",
    "\"\"\"\n",
    "import glob\n",
    "from WA.LCC_to_LUWA import Rasterize_shape_basin,Adjust_GRaND_reservoir,Reclass_LCC_to_LUWA\n",
    "\n",
    "### Get all yearly WaPOR LCC maps\n",
    "LCC_fhs=glob.glob(os.path.join(BASIN['input_data']['yearly']['lcc'][0],'*.tif'))\n",
    "WaPOR_LCC=LCC_fhs[0] #template\n",
    "\n",
    "### Create Reservoir raster map for basin\n",
    "Dir_out=os.path.join(BASIN['Dir'],'data','Reservoir')\n",
    "if not os.path.exists(Dir_out):\n",
    "    os.makedirs(Dir_out)\n",
    "Basin_Reservoir_tif=os.path.join(Dir_out,'Reservoir_basin.tif')\n",
    "Global_GRaND_Reservoir=BASIN['global_data']['grand']\n",
    "\n",
    "# adjust reservoir\n",
    "Resrv_to_Lake=BASIN['geo_data']['unreserv']\n",
    "Lake_to_Reserv=BASIN['geo_data']['makereserv']  \n",
    "if (Resrv_to_Lake!=None)&(Lake_to_Reserv!=None): #require 2 shapefiles to unreservoir and makereservoir           \n",
    "    Adjust_GRaND_reservoir(Basin_Reservoir_tif,WaPOR_LCC,\n",
    "                           Global_GRaND_Reservoir,Resrv_to_Lake,Lake_to_Reserv)    \n",
    "else:                \n",
    "    Rasterize_shape_basin(Global_GRaND_Reservoir,WaPOR_LCC,Basin_Reservoir_tif)\n",
    "    \n",
    "BASIN['geo_data']['reservoir']=Basin_Reservoir_tif\n",
    "\n",
    "### Create Protected Area raster map for basin\n",
    "Dir_out=os.path.join(BASIN['Dir'],'data','Protected')\n",
    "if not os.path.exists(Dir_out):\n",
    "    os.makedirs(Dir_out)\n",
    "ProtectedArea_tif=os.path.join(Dir_out,'ProtectedArea_basin.tif')\n",
    "WDPA=BASIN['global_data']['wdpa']\n",
    "Rasterize_shape_basin(WDPA,WaPOR_LCC,ProtectedArea_tif)\n",
    "\n",
    "### Reclassify LCC to LUWA       \n",
    "Dir_out=os.path.join(BASIN['Dir'],'data','luwa')\n",
    "if not os.path.exists(Dir_out):\n",
    "    os.makedirs(Dir_out)\n",
    "for fh in LCC_fhs:\n",
    "    Reclass_LCC_to_LUWA(fh,Dir_out,ProtectedArea_tif,Basin_Reservoir_tif)\n",
    "\n",
    "# Add LUWA folder to basin metadata\n",
    "BASIN['input_data']['yearly']['lu']=[Dir_out,'-','LUWA Categories']\n",
    "pickle_out(BASIN)              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create netCDF for each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.806621238863997e-16\n",
      "-6.806621238863997e-16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:07<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished lcc_yearly.nc\n",
      "6.806621238863997e-16\n",
      "-6.806621238863997e-16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:06<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished lu_yearly.nc\n",
      "6.806621238863997e-16\n",
      "-6.806621238863997e-16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 120/120 [00:49<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished p_monthly.nc\n",
      "6.806621238863997e-16\n",
      "-6.806621238863997e-16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 120/120 [01:36<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished et_monthly.nc\n",
      "6.806621238863997e-16\n",
      "-6.806621238863997e-16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 120/120 [00:38<00:00,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished ret_monthly.nc\n",
      "6.806621238863997e-16\n",
      "-6.806621238863997e-16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 120/120 [01:13<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished i_monthly.nc\n",
      "6.806621238863997e-16\n",
      "-6.806621238863997e-16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 120/120 [00:51<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished nRD_monthly.nc\n",
      "6.806621238863997e-16\n",
      "-6.806621238863997e-16\n",
      "Finished thetasat_topsoil.tif\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'D:\\\\FAO\\\\WA_Sheet1\\\\Main\\\\Litani\\\\Info_20191108_10h11.pickle'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from WA.create_NC import main as make_netcdf\n",
    "from WA.create_NC import _get_lats_lons \n",
    "import gdal\n",
    "import tempfile\n",
    "\n",
    "#Get inputs for create_NC\n",
    "cutline=BASIN['geo_data']['basin']\n",
    "Dir_out=os.path.join(BASIN['Dir'],'data','nc')\n",
    "if not os.path.exists(Dir_out):\n",
    "    os.makedirs(Dir_out)    \n",
    "template=glob.glob(os.path.join(BASIN['input_data']['yearly']['lcc'][0],'*.tif'))[0]          \n",
    "    \n",
    "### yearly maps\n",
    "for key in BASIN['input_data']['yearly']:             \n",
    "    if BASIN['input_data']['yearly'][key] is not None:\n",
    "        nc_fn=os.path.join(Dir_out,key+'_yearly.nc')        \n",
    "        dataset={key:[BASIN['input_data']['yearly'][key][0],\n",
    "                       ('time','latitude', 'longitude'), \n",
    "                       {'units': BASIN['input_data']['yearly'][key][1],                                 \n",
    "                        'quantity':BASIN['input_data']['yearly'][key][2],\n",
    "                        'source': 'WaPOR', 'period':'year'}]}\n",
    "        succes=make_netcdf(nc_fn,BASIN['Name'],dataset,template,cutline,step='year')\n",
    "        if succes:\n",
    "            BASIN['main_data']['yearly'][key]=nc_fn\n",
    "            print('Finished {0}_yearly.nc'.format(key))\n",
    "            \n",
    "### monthly maps  \n",
    "for key in BASIN['input_data']['monthly']:           \n",
    "    if BASIN['input_data']['monthly'][key] is not None:\n",
    "        nc_fn=os.path.join(Dir_out,key+'_monthly.nc')                \n",
    "        dataset={key:[BASIN['input_data']['monthly'][key][0],\n",
    "                       ('time','latitude', 'longitude'), \n",
    "                       {'units': BASIN['input_data']['monthly'][key][1],                                 \n",
    "                        'quantity':BASIN['input_data']['monthly'][key][2],\n",
    "                        'source': 'WaPOR', 'period':'month'}]}\n",
    "        succes=make_netcdf(nc_fn,BASIN['Name'],dataset,template,cutline,step='month')\n",
    "        if succes:\n",
    "            BASIN['main_data']['monthly'][key]=nc_fn\n",
    "            print('Finished {0}_monthly.nc'.format(key))\n",
    "  \n",
    "### static maps\n",
    "lats, lons, optionsProj, optionsClip =_get_lats_lons(template,cutline)\n",
    "Dir_out=os.path.join(BASIN['Dir'],'data','stat')\n",
    "if not os.path.exists(Dir_out):\n",
    "    os.makedirs(Dir_out)\n",
    "for key in BASIN['input_data']['stat']:            \n",
    "    if BASIN['input_data']['stat'][key] is not None:  \n",
    "        basename= os.path.basename(BASIN['input_data']['stat'][key])\n",
    "        outfn=os.path.join(Dir_out,basename)  \n",
    "        temp_file = tempfile.NamedTemporaryFile(suffix='.tif').name               \n",
    "        gdal.Warp(temp_file, BASIN['input_data']['stat'][key], options = optionsProj)\n",
    "        gdal.Warp(outfn, temp_file, options = optionsClip)\n",
    "        os.remove(temp_file)\n",
    "        BASIN['main_data']['stat'][key]=outfn \n",
    "        print('Finished {0}'.format(basename))\n",
    "\n",
    "pickle_out(BASIN)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Correct ET of water bodies by ET reference \n",
    "**If ETa of water bodies is underestimated**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Mon Sep  2 19:45:28 2019\n",
    "\n",
    "@author: sse\n",
    "\"\"\"\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def open_nc(nc,timechunk=1,chunksize=1000):\n",
    "    dts=xr.open_dataset(nc)\n",
    "    key=list(dts.keys())[0]\n",
    "    var=dts[key].chunk({\"time\": timechunk, \"latitude\": chunksize, \"longitude\": chunksize}) #.ffill(\"time\")\n",
    "    return var,key\n",
    "\n",
    "#%% replace Et of water bodies by ET reference (based on FAO's recommendation)\n",
    "#lu\n",
    "lcc,_=open_nc(BASIN['main_data']['yearly']['lcc'])\n",
    "#ET\n",
    "et,_ = open_nc(BASIN['main_data']['monthly']['et'])\n",
    "#RET\n",
    "ret,_= open_nc(BASIN['main_data']['monthly']['ret'])\n",
    "\n",
    "lcc_dict = {\n",
    "                20: 'Shrubland',\n",
    "                30: 'Grassland', \n",
    "                41: 'Cropland, rainfed',\n",
    "                42: 'Cropland, irrigated or under water management', \n",
    "                43: 'Cropland, fallow', \n",
    "                50: 'Built-up', \n",
    "                60: 'Bare / sparse vegetation', \n",
    "                70: 'Permament snow / ice', \n",
    "                80: 'Water bodies', \n",
    "                81: 'Temporary water bodies', \n",
    "                90: 'Shrub or herbaceous cover, flooded', \n",
    "                111: 'Tree cover: closed, evergreen needle-leaved', \n",
    "                112: 'Tree cover: closed, evergreen broadleaved', \n",
    "                114: 'Tree cover: closed, deciduous broadleaved', \n",
    "                115: 'Tree cover: closed, mixed type', \n",
    "                116: 'Tree cover: closed, unknown type', \n",
    "                121: 'Tree cover: open, evergreen needle-leaved', \n",
    "                122: 'Tree cover: open, evergreen broadleaved', \n",
    "                123: 'Tree cover: open, deciduous needle-leaved', \n",
    "                124: 'Tree cover: open, deciduous broadleaved', \n",
    "                125: 'Tree cover: open, mixed type',\n",
    "                126: 'Tree cover: open, unknown type', \n",
    "                200: 'Sea water'\n",
    "          }\n",
    "##\n",
    "et_lcc_t = []\n",
    "\n",
    "for i in range(len(lcc.time)): \n",
    "    t1 = i*12\n",
    "    t2 = (i+1)*12\n",
    "    lcc_t = lcc.isel(time = i)\n",
    "    et_m_t = []\n",
    "    for t in range(t1,t2):\n",
    "        et_t = et.isel(time = t)  \n",
    "        ret_t = ret.isel(time = t)  \n",
    "        et_updated_t=et_t.where((lcc_t!=80)&(lcc_t!=200),ret_t)\n",
    "        #et_updated_t.assign_coords({'time': et_t.time})\n",
    "        et_updated_t = et_updated_t.assign_coords(time=et_t.time)\n",
    "        et_updated_t = et_updated_t.expand_dims('time')\n",
    "        et_m_t.append(et_updated_t)\n",
    "    et_m = xr.concat(et_m_t, dim = 'time')\n",
    "    et_lcc_t.append(et_m)\n",
    "et_modified= xr.concat(et_lcc_t, dim='time')    \n",
    "et_modified=et_modified.chunk({\"latitude\":-1,\"longitude\":-1}).to_dataset()\n",
    "\n",
    "### save corrected ETa as netCDF\n",
    "nc_fn=r\"et_corrected_monthly.nc\"\n",
    "outfolder = os.path.join(BASIN['Dir'],'data','nc')\n",
    "nc_path=os.path.join(outfolder,nc_fn)\n",
    "et_modified.to_netcdf(nc_path,encoding={\"Actual Evapotranspiration\":{'zlib':True}})\n",
    "BASIN['main_data']['monthly']['et']=nc_path\n",
    "pickle_out(BASIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Pixel-analysis: Compute ET$_{incr}$ and ET$_{rain}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Read Input from Basin metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parameters\n",
    "start_year=2009\n",
    "f_perc=1 # percolation factor\n",
    "f_Smax=0.9\n",
    "cf =  20 #f_Ssat soil mositure correction factor to componsate the variation in filling up and drying in a month\n",
    "    \n",
    "## input data\n",
    "smsat_file = BASIN['main_data']['stat']['key']\n",
    "## read nc files    \n",
    "p_in = BASIN['main_data']['monthly']['p']\n",
    "e_in = BASIN['main_data']['monthly']['et']\n",
    "i_in = BASIN['main_data']['monthly']['i']\n",
    "rd_in = BASIN['main_data']['monthly']['nRD']\n",
    "lu_in = BASIN['main_data']['yearly']['lcc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Run Monthly Soil moisture Balance Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start a status Dashboard of dasked array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client=Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Oct 18 08:22:17 2019\n",
    "\n",
    "@author: Solomon Seyoum\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import gdal\n",
    "import xarray as xr\n",
    "\n",
    "#%%\n",
    "def open_nc(nc,chunksize=1000):\n",
    "    dts=xr.open_dataset(nc)\n",
    "    key=list(dts.keys())[0]\n",
    "    var=dts[key].chunk({\"time\": 1, \"latitude\": chunksize, \"longitude\": chunksize}) #.ffill(\"time\")\n",
    "    return var,key\n",
    "\n",
    "def SM_bucket(P,ETa,I,SMmax,SMgt_1,SMincrt_1,f_consumed):\n",
    "    SMt_1=SMgt_1+SMincrt_1\n",
    "    ETr=np.where(I>0,ETa-I, ETa)\n",
    "    SMtemp=SMgt_1+np.maximum(P-I,P*0)\n",
    "    SMg=np.where(SMtemp-ETr>0,SMtemp-ETr,P*0)\n",
    "    ETincr=np.where(SMtemp-ETr>0,P*0,ETr-SMtemp)\n",
    "    Qsupply=np.where(ETincr>SMincrt_1,(ETincr-SMincrt_1)/f_consumed, P*0) \n",
    "            \n",
    "    #SMincr=np.where(ETincr>SMincrt_1,SMincrt_1+Qsupply-ETincr,SMincrt_1-ETincr)\n",
    "    SMincr=SMincrt_1+Qsupply-ETincr\n",
    "    SMincr=np.where(SMg+SMincr>SMmax,SMincr-SMincr/(SMg+SMincr)*((SMg+SMincr)-SMmax),SMincr)     \n",
    "    SMg=np.where(SMg+SMincr>SMmax,(SMg-SMg/(SMg+SMincr)*((SMg+SMincr)-SMmax)),SMg)\n",
    "    \n",
    "    #SMincr=np.where(SMg+SMincr>SMmax,SMincr-((SMg+SMincr)-SMmax),SMincr)\n",
    "    SM=np.where(SMg+SMincr>SMmax,SMmax,SMg+SMincr)\n",
    "    dsm=SM-SMt_1\n",
    "    ETg=ETa-ETincr\n",
    "    return SMg,SMincr,SM,dsm,Qsupply,ETincr,ETg\n",
    "\n",
    "\n",
    "def get_rootdepth(version = '1.0'):\n",
    "    \n",
    "    lcc_code = dict()\n",
    "    lcc_code['1.0'] = {\n",
    "       'Shrubland':20,\n",
    "       'Grassland':30,\n",
    "       'Rainfed Cropland':41,\n",
    "       'Irrigated Cropland':42,\n",
    "       'Fallow Cropland':43,\n",
    "       'Built-up':50,\n",
    "       'Bare/sparse vegetation':60,\n",
    "       'Permanent snow/ ice':70,\n",
    "       'Water bodies':80,\n",
    "       'Temporary water bodies':81,\n",
    "       'Shrub or herbaceous cover, flooded':90,     \n",
    "       'Tree cover: closed, evergreen needle-leaved':111,\n",
    "       'Tree cover: closed, evergreen broad-leaved':112, \n",
    "       'Tree cover: closed, deciduous broad-leaved':114, #\n",
    "       'Tree cover: closed, mixed type':115, #\n",
    "       'Tree cover: closed, unknown type':116, #\n",
    "       'Tree cover: open, evergreen needle-leaved':121,#\n",
    "       'Tree cover: open, evergreen broad-leaved':122, #\n",
    "       'Tree cover: open, deciduous needle-leaved':123, #\n",
    "       'Tree cover: open, deciduous broad-leaved':124, #\n",
    "       'Tree cover: open, mixed type':125, #\n",
    "       'Tree cover: open, unknown type':126, #     \n",
    "       'Seawater':200, #       \n",
    "       }\n",
    "    \n",
    "    root_depth = dict()\n",
    "    '''\n",
    "    based on Global estimation of effective plant rooting depth: \n",
    "    Implications for hydrological modeling by Yang et al (2016)\n",
    "    '''\n",
    "    root_depth['1.0'] = {\n",
    "       'Shrubland':370,\n",
    "       'Grassland':510,\n",
    "       'Rainfed Cropland':550,\n",
    "       'Irrigated Cropland':550,\n",
    "       'Fallow Cropland':550,\n",
    "       'Built-up':370,\n",
    "       'Bare/sparse vegetation':370,\n",
    "       'Permanent snow/ ice':0,\n",
    "       'Water bodies':0,\n",
    "       'Temporary water bodies':0,\n",
    "       'Shrub or herbaceous cover, flooded':0,     \n",
    "       'Tree cover: closed, evergreen needle-leaved':1800,\n",
    "       'Tree cover: closed, evergreen broad-leaved':3140, \n",
    "       'Tree cover: closed, deciduous broad-leaved':1070, #\n",
    "       'Tree cover: closed, mixed type':2000, #\n",
    "       'Tree cover: closed, unknown type':2000, #\n",
    "       'Tree cover: open, evergreen needle-leaved':1800,#\n",
    "       'Tree cover: open, evergreen broad-leaved':3140, #\n",
    "       'Tree cover: open, deciduous needle-leaved':1070, #\n",
    "       'Tree cover: open, deciduous broad-leaved':1070, #\n",
    "       'Tree cover: open, mixed type':2000, #\n",
    "       'Tree cover: open, unknown type':2000, #     \n",
    "       'Seawater':0, #       \n",
    "    }\n",
    "    \n",
    "    return lcc_code[version], root_depth[version]\n",
    "\n",
    "def root_dpeth(lu):\n",
    "    rootdepth=np.copy(lu)\n",
    "    lu_categories, root_depth = get_rootdepth(version = '1.0')\n",
    "    for key in root_depth.keys():\n",
    "        mask = np.logical_or.reduce([lu == lu_categories[key]])\n",
    "        rd = root_depth[key]\n",
    "        rootdepth[mask] = rd\n",
    "    return rootdepth \n",
    "\n",
    "\n",
    "def get_fractions(version = '1.0'):\n",
    "    consumed_fractions = dict()\n",
    "    \n",
    "    consumed_fractions['1.0'] = {\n",
    "       'Shrubland':1.00,\n",
    "       'Grassland':1.00,\n",
    "       'Rainfed Cropland':1.00,\n",
    "       'Irrigated Cropland':0.80,\n",
    "       'Fallow Cropland':1.00,\n",
    "       'Built-up':1.00,\n",
    "       'Bare/sparse vegetation':1.00,\n",
    "       'Permanent snow/ ice':1.00,\n",
    "       'Water bodies':1.00,\n",
    "       'Temporary water bodies':1.00,\n",
    "       'Shrub or herbaceous cover, flooded':1.00,     \n",
    "       'Tree cover: closed, evergreen needle-leaved':1.00,\n",
    "       'Tree cover: closed, evergreen broad-leaved':1.00, \n",
    "       'Tree cover: closed, deciduous broad-leaved':1.00, #\n",
    "       'Tree cover: closed, mixed type':1.00, #\n",
    "       'Tree cover: closed, unknown type':1.00, #\n",
    "       'Tree cover: open, evergreen needle-leaved':1.00,#\n",
    "       'Tree cover: open, evergreen broad-leaved':1.00, #\n",
    "       'Tree cover: open, deciduous needle-leaved':1.00, #\n",
    "       'Tree cover: open, deciduous broad-leaved':1.00, #\n",
    "       'Tree cover: open, mixed type':1.00, #\n",
    "       'Tree cover: open, unknown type':1.00, #     \n",
    "       'Seawater':1.00, #       \n",
    "#            'Forests':              1.00,\n",
    "#            'Shrubland':            1.00,\n",
    "#            'Rainfed Crops':        1.00,\n",
    "#            'Forest Plantations':   1.00,\n",
    "#            'Natural Water Bodies': 1.00,\n",
    "#            'Wetlands':             1.00,\n",
    "#            'Natural Grasslands':   1.00,\n",
    "#            'Other (Non-Manmade)':  1.00,\n",
    "#            'Irrigated crops':      0.80,\n",
    "#            'Managed water bodies': 1.00,\n",
    "#            'Other':                1.00,\n",
    "#            'Residential':          1.00,\n",
    "#            'Greenhouses':          0.95,\n",
    "#            'Aquaculture':          1.00\n",
    "    }\n",
    "    \n",
    "    return consumed_fractions[version]\n",
    "\n",
    "def Consumed_fraction(lu):\n",
    "    f_consumed=np.copy(lu)\n",
    "    consumed_fractions = get_fractions(version = '1.0')\n",
    "    lu_categories, root_depth = get_rootdepth(version = '1.0')\n",
    "    for key in consumed_fractions.keys():\n",
    "        mask = np.logical_or.reduce([lu == lu_categories[key]])\n",
    "        consumed_fraction = consumed_fractions[key]\n",
    "        f_consumed[mask] = consumed_fraction\n",
    "    return f_consumed \n",
    "\n",
    "#%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  0\n",
      "time:  1\n",
      "time:  2\n",
      "time:  3\n",
      "time:  4\n",
      "time:  5\n",
      "time:  6\n",
      "time:  7\n",
      "time:  8\n",
      "time:  9\n",
      "time:  10\n",
      "time:  11\n",
      "time:  12\n",
      "time:  13\n",
      "time:  14\n",
      "time:  15\n",
      "time:  16\n",
      "time:  17\n",
      "time:  18\n",
      "time:  19\n",
      "time:  20\n",
      "time:  21\n",
      "time:  22\n",
      "time:  23\n",
      "time:  24\n",
      "time:  25\n",
      "time:  26\n",
      "time:  27\n",
      "time:  28\n",
      "time:  29\n",
      "time:  30\n",
      "time:  31\n",
      "time:  32\n",
      "time:  33\n",
      "time:  34\n",
      "time:  35\n",
      "time:  36\n",
      "time:  37\n",
      "time:  38\n",
      "time:  39\n",
      "time:  40\n",
      "time:  41\n",
      "time:  42\n",
      "time:  43\n",
      "time:  44\n",
      "time:  45\n",
      "time:  46\n",
      "time:  47\n",
      "time:  48\n",
      "time:  49\n",
      "time:  50\n",
      "time:  51\n",
      "time:  52\n",
      "time:  53\n",
      "time:  54\n",
      "time:  55\n",
      "time:  56\n",
      "time:  57\n",
      "time:  58\n",
      "time:  59\n",
      "time:  60\n",
      "time:  61\n",
      "time:  62\n",
      "time:  63\n",
      "time:  64\n",
      "time:  65\n",
      "time:  66\n",
      "time:  67\n",
      "time:  68\n",
      "time:  69\n",
      "time:  70\n",
      "time:  71\n",
      "time:  72\n",
      "time:  73\n",
      "time:  74\n",
      "time:  75\n",
      "time:  76\n",
      "time:  77\n",
      "time:  78\n",
      "time:  79\n",
      "time:  80\n",
      "time:  81\n",
      "time:  82\n",
      "time:  83\n",
      "time:  84\n",
      "time:  85\n",
      "time:  86\n",
      "time:  87\n",
      "time:  88\n",
      "time:  89\n",
      "time:  90\n",
      "time:  91\n",
      "time:  92\n",
      "time:  93\n",
      "time:  94\n",
      "time:  95\n",
      "time:  96\n",
      "time:  97\n",
      "time:  98\n",
      "time:  99\n",
      "time:  100\n",
      "time:  101\n",
      "time:  102\n",
      "time:  103\n",
      "time:  104\n",
      "time:  105\n",
      "time:  106\n",
      "time:  107\n",
      "time:  108\n",
      "time:  109\n",
      "time:  110\n",
      "time:  111\n",
      "time:  112\n",
      "time:  113\n",
      "time:  114\n",
      "time:  115\n",
      "time:  116\n",
      "time:  117\n",
      "time:  118\n",
      "time:  119\n",
      "Wall time: 3min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Pt,key=open_nc(p_in,chunksize=1000)\n",
    "E,key=open_nc(e_in,chunksize=1000)\n",
    "Int,key=open_nc(i_in,chunksize=1000)\n",
    "nRD,key=open_nc(rd_in,chunksize=1000)\n",
    "LU,key=open_nc(lu_in,chunksize=1000)\n",
    "\n",
    "##read saturation file\n",
    "ds = gdal.Open(smsat_file)\n",
    "thetasat = ds.ReadAsArray()\n",
    "\n",
    "nrd = nRD.where(nRD!=0,1)\n",
    " \n",
    "SMg=E[0]*0\n",
    "SMincr=E[0]*0\n",
    "\n",
    "for j in range(len(LU.time)): \n",
    "    t1 = j*12\n",
    "    t2 = (j+1)*12    \n",
    "    etb = np.zeros((12,E[0].shape[0],E[0].shape[1]))\n",
    "    etg = np.zeros((12,E[0].shape[0],E[0].shape[1]))\n",
    "    lu = LU.isel(time=j).values\n",
    "    Rd = root_dpeth(lu)\n",
    "    SMmax=thetasat*Rd    \n",
    "    f_consumed = Consumed_fraction(lu)    \n",
    "    for t in range(t1,t2):\n",
    "        print('time: ', t)\n",
    "        if t==0:\n",
    "            SMgt_1=E[0]*0\n",
    "            Qsupply=E[0]*0\n",
    "            SMincrt_1=E[0]*0\n",
    "        else:\n",
    "            SMgt_1=SMg \n",
    "            SMincrt_1=SMincr         \n",
    "        p = Pt.isel(time=t).values\n",
    "        e = E.isel(time=t).values\n",
    "        i = Int.isel(time=t).values\n",
    "        NRD = nrd.isel(time=t).values\n",
    "        P = p/NRD\n",
    "        ETa = e/NRD\n",
    "        I = i/NRD\n",
    "        \n",
    "        ### Step 1: Soil moisture\n",
    "        SMg,SMincr,SM,dsm,Qsupply,ETincr,ETg=SM_bucket(P,ETa,I,SMmax,SMgt_1,SMincrt_1,f_consumed) \n",
    "    \n",
    "        ### Step 3: Percolation        \n",
    "        perc_fromSM=np.where(SM>0.6*SMmax,(1-np.exp(-f_perc/SM)),P*0)\n",
    "        SM = SM - perc_fromSM\n",
    "        SMt_1= SMgt_1+SMincrt_1\n",
    "        dsm = SM-SMt_1\n",
    "        \n",
    "        ### Step 5: Store monthly data of the year\n",
    "        k = int(t-(j*12))\n",
    "        etb[k,:,:] = ETincr*NRD\n",
    "        etg[k,:,:] = ETg*NRD\n",
    "        \n",
    "        del p\n",
    "        del ETa\n",
    "        del I\n",
    "        del NRD\n",
    "        del SMgt_1 \n",
    "        del Qsupply \n",
    "        del SMincrt_1\n",
    "        \n",
    "    year = start_year+j    \n",
    "#    E['etb'] = (('time','latitude', 'longitude'), etb)\n",
    "#    E['etg'] = (('time','latitude', 'longitude'), etg)\n",
    "    time_ds = E.time[t1:t2]\n",
    "    ds = xr.Dataset({})\n",
    "    ds['etb'] = (('time','latitude', 'longitude'), etb)\n",
    "    ds['etg'] = (('time','latitude', 'longitude'), etg)\n",
    "    ds = ds.assign_coords(time = time_ds, latitude=E.latitude,longitude=E.longitude )\n",
    "    \n",
    "    ##green ET datase\n",
    "    et_g = ds.etg\n",
    "    attrs={\"units\":\"mm/month\", \"source\": \"FAO WaPOR\", \"quantity\":\"Rainfall_ET_M\"}\n",
    "    et_g.assign_attrs(attrs)\n",
    "    et_g.name = \"Rainfall_ET_M\"\n",
    "    et_g_dts=et_g.chunk({\"latitude\":-1,\"longitude\":-1}).to_dataset()\n",
    "    outfolder=os.path.join(Basin['Dir'],'data','nc','etg')\n",
    "    nc_fn=r'et_g_monthly_'+str(year)+'.nc'\n",
    "    nc_path=os.path.join(outfolder,nc_fn)\n",
    "    comp = dict(zlib=True, complevel=9, least_significant_digit=3)\n",
    "    encoding = {\"Rainfall_ET_M\": comp}\n",
    "    et_g_dts.to_netcdf(nc_path,encoding=encoding)\n",
    "    \n",
    "    ##Blue ET datase\n",
    "    et_b = ds.etb\n",
    "    attrs={\"units\":\"mm/month\", \"source\": \"FAO WaPOR\", \"quantity\":\"Increamental_ET_M\"}\n",
    "    et_b.assign_attrs(attrs)\n",
    "    et_b.name = \"Increamental_ET_M\"\n",
    "    et_b_dts=et_b.chunk({\"latitude\":-1,\"longitude\":-1}).to_dataset()\n",
    "    outfolder=os.path.join(Basin['Dir'],'data','nc','etb')\n",
    "    nc_fn=r'et_b_monthly_'+str(year)+'.nc'\n",
    "    nc_path=os.path.join(outfolder,nc_fn)\n",
    "    et_b_dts.to_netcdf(nc_path,encoding={\"Increamental_ET_M\":{'zlib':True}})\n",
    "    \n",
    "    del etb\n",
    "    del etg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Merge monthly ET$_{rain}$ and ET$_{incr}$ netcdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:\\\\FAO\\\\WA_Sheet1\\\\Main\\\\Litani\\\\data\\\\nc\\\\etg\\\\et_g_monthly_2009.nc',\n",
       " 'D:\\\\FAO\\\\WA_Sheet1\\\\Main\\\\Litani\\\\data\\\\nc\\\\etg\\\\et_g_monthly_2010.nc',\n",
       " 'D:\\\\FAO\\\\WA_Sheet1\\\\Main\\\\Litani\\\\data\\\\nc\\\\etg\\\\et_g_monthly_2011.nc',\n",
       " 'D:\\\\FAO\\\\WA_Sheet1\\\\Main\\\\Litani\\\\data\\\\nc\\\\etg\\\\et_g_monthly_2012.nc',\n",
       " 'D:\\\\FAO\\\\WA_Sheet1\\\\Main\\\\Litani\\\\data\\\\nc\\\\etg\\\\et_g_monthly_2013.nc',\n",
       " 'D:\\\\FAO\\\\WA_Sheet1\\\\Main\\\\Litani\\\\data\\\\nc\\\\etg\\\\et_g_monthly_2014.nc',\n",
       " 'D:\\\\FAO\\\\WA_Sheet1\\\\Main\\\\Litani\\\\data\\\\nc\\\\etg\\\\et_g_monthly_2015.nc',\n",
       " 'D:\\\\FAO\\\\WA_Sheet1\\\\Main\\\\Litani\\\\data\\\\nc\\\\etg\\\\et_g_monthly_2016.nc',\n",
       " 'D:\\\\FAO\\\\WA_Sheet1\\\\Main\\\\Litani\\\\data\\\\nc\\\\etg\\\\et_g_monthly_2017.nc',\n",
       " 'D:\\\\FAO\\\\WA_Sheet1\\\\Main\\\\Litani\\\\data\\\\nc\\\\etg\\\\et_g_monthly_2018.nc']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nc_folder=r\"D:\\FAO\\WA_Sheet1\\Main\\Litani\\data\\nc\\etg\"\n",
    "fhs=glob.glob(os.path.join(nc_folder,'*.nc'))\n",
    "fhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'Rainfall_ET_M' (time: 120, latitude: 958, longitude: 1181)>\n",
       "dask.array<shape=(120, 958, 1181), dtype=float64, chunksize=(1, 958, 1000)>\n",
       "Coordinates:\n",
       "  * latitude   (latitude) float32 34.049603 34.04861 ... 33.10119 33.100197\n",
       "  * longitude  (longitude) float32 35.229168 35.23016 ... 36.39881 36.399803\n",
       "  * time       (time) datetime64[ns] 2009-01-01 2009-02-01 ... 2018-12-01"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_list=[]\n",
    "for fh in fhs:\n",
    "    arr,key=open_nc(fh)\n",
    "    arr_list.append(arr)\n",
    "ETg=xr.concat(arr_list, dim='time')\n",
    "ETg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs={\"units\":\"mm/month\", \"source\": \"FAO WaPOR\", \"quantity\":\"Rainfall_ET_M\"}\n",
    "ETg.assign_attrs(attrs)\n",
    "ETg_dts=ETg.chunk({\"latitude\":-1,\"longitude\":-1}).to_dataset()\n",
    "outfolder=os.path.join(Basin['Dir'],'data','nc')\n",
    "nc_fn=r'etg_monthly.nc'\n",
    "nc_path=os.path.join(outfolder,nc_fn)\n",
    "comp = dict(zlib=True, complevel=9, least_significant_digit=3)\n",
    "encoding = {\"Rainfall_ET_M\": comp}\n",
    "ETg_dts.to_netcdf(nc_path,encoding=encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_folder=r\"D:\\FAO\\WA_Sheet1\\Main\\Litani\\data\\nc\\etb\"\n",
    "fhs=glob.glob(os.path.join(nc_folder,'*.nc'))\n",
    "arr_list=[]\n",
    "for fh in fhs:\n",
    "    arr,key=open_nc(fh)\n",
    "    arr_list.append(arr)\n",
    "ETb=xr.concat(arr_list, dim='time')\n",
    "attrs={\"units\":\"mm/month\", \"source\": \"FAO WaPOR\", \"quantity\":\"Incremental_ET_M\"}\n",
    "ETb.assign_attrs(attrs)\n",
    "ETb_dts=ETb.chunk({\"latitude\":-1,\"longitude\":-1}).to_dataset()\n",
    "outfolder=os.path.join(Basin['Dir'],'data','nc')\n",
    "nc_fn=r'etb_monthly.nc'\n",
    "nc_path=os.path.join(outfolder,nc_fn)\n",
    "comp = dict(zlib=True, complevel=9, least_significant_digit=3)\n",
    "encoding = {\"Increamental_ET_M\": comp}\n",
    "ETb_dts.to_netcdf(nc_path,encoding=encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\FAO\\\\WA_Sheet1\\\\Main\\\\Litani\\\\Info_20191106_11h33.pickle'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Basin['main_data']['monthly']['etg']=r\"D:\\FAO\\WA_Sheet1\\Main\\Litani\\data\\nc\\etg_monthly.nc\"\n",
    "Basin['main_data']['monthly']['etb']=r\"D:\\FAO\\WA_Sheet1\\Main\\Litani\\data\\nc\\etb_monthly.nc\"\n",
    "pickle_out(Basin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Aggregate annual value by hydrological year\n",
    "**Variables: ET (water bodies corrected with ETref), ETincr, ETrain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time='A-{0}'.format(BASIN['end_month'])\n",
    "for key in ['p','et','etb','etg']:\n",
    "    nc=Basin['main_data']['monthly'][key]\n",
    "    var,name=open_nc(nc)\n",
    "    var_y=var.resample(time=time).sum(dim='time',skipna=False)\n",
    "    outfolder=os.path.join(Basin['Dir'],'data','nc')  \n",
    "    attrs={\"units\":\"mm/year\", \"source\": \"Hydrolological year Sep-Aug\", \"quantity\":name}\n",
    "    var_y.assign_attrs(attrs)\n",
    "    var_y.name = name\n",
    "    var_y_dts=var_y.chunk({\"latitude\":-1,\"longitude\":-1}).to_dataset()\n",
    "    nc_fn=\"{0}_hyearly1.nc\".format(key)\n",
    "    nc_path=os.path.join(outfolder,nc_fn)\n",
    "    var_y_dts.to_netcdf(nc_path,encoding={name:{'zlib':True}})\n",
    "    Basin['main_data']['yearly'][key]=nc_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Calculate fluxes volumes table for Sheet 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Area mask\n",
    "# from WA import GIS_functions as gis\n",
    "\n",
    "# # get template from LU map\n",
    "# template=glob.glob(os.path.join(Basin['input_data']['yearly']['lu'][0],\"*.tif\"))[0]\n",
    "# driver, NDV, xsize, ysize, GeoT, Projection = gis.GetGeoInfo(template)\n",
    "# # calculate area per pixel\n",
    "# area_map=gis.MapPixelAreakm(template)\n",
    "# # save area map as tif\n",
    "# Basin['input_data']['stat']['area']=os.path.join(Basin['Dir'],'data','stat','Area_km.tif')\n",
    "# gis.CreateGeoTiff(Basin['input_data']['stat']['area'],area_map,driver, NDV, xsize, ysize, GeoT, Projection)\n",
    "\n",
    "# # create area mask\n",
    "# shape=Basin['geo_data']['basin']\n",
    "# area=Basin['input_data']['stat']['area']\n",
    "# Basin['main_data']['stat']['area']=os.path.join(Basin['Dir'],'data','stat','Basin_Area_km.tif')\n",
    "# gis.Clip_shapefile(area,shape,Basin['main_data']['stat']['area'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Region of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from WA import GIS_functions as gis\n",
    "import xarray as xr\n",
    "area_fh=Basin['main_data']['stat']['area']\n",
    "# area_fh=Basin['main_data']['subbasin']['Zarqa']\n",
    "area=gis.OpenAsArray(area_fh,nan_values=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate P, ET, ETb, ETg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_all=[] #all timeseries\n",
    "for key in ['p','et','etg','etb']:\n",
    "    nc=Basin['main_data']['yearly'][key]\n",
    "    dts=xr.open_dataset(nc)\n",
    "    var_name=list(dts.keys())[0]\n",
    "    var=dts[var_name].chunk({\"time\": 1, \"latitude\": 1000, \"longitude\": 1000}).ffill(\"time\")\n",
    "    #attrs=dts[var_name].attrs\n",
    "    Volume=var*area\n",
    "    #attrs['units']='TCM/year'\n",
    "    #attrs['quantity']='Precipitation (TCM/year)'\n",
    "    #Volume.assign_attrs(attrs)\n",
    "    ts=Volume.sum(dim=['latitude','longitude']).to_dataframe()\n",
    "    ts.index=[y.year for y in ts.index]\n",
    "    ts_all.append(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate ETb, ETg per LU category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LU_dts=xr.open_dataset(Basin['main_data']['yearly']['lu'])\n",
    "ETg_dts=xr.open_dataset(Basin['main_data']['yearly']['etg'])\n",
    "ETb_dts=xr.open_dataset(Basin['main_data']['yearly']['etb'])\n",
    "\n",
    "ETg=ETg_dts[\"Rainfall_ET_M\"].chunk({\"time\": 1, \"latitude\": 1000, \"longitude\": 1000}).ffill(\"time\")\n",
    "ETb=ETb_dts[\"Increamental_ET_M\"].chunk({\"time\": 1, \"latitude\": 1000, \"longitude\": 1000}).ffill(\"time\")\n",
    "LU=LU_dts[\"LUWA Categories\"].chunk({\"time\": 1, \"latitude\": 1000, \"longitude\": 1000}).ffill(\"time\")\n",
    "\n",
    "### Different year date to year \n",
    "LU=LU.groupby('time.year').first(skipna=False)\n",
    "ETb=area*ETb.groupby('time.year').first(skipna=False)\n",
    "ETg=area*ETg.groupby('time.year').first(skipna=False)\n",
    "### average per LU\n",
    "from WA.average_by_LU import Total_perLU\n",
    "ts_ETb=Total_perLU(ETb,LU)\n",
    "ts_ETg=Total_perLU(ETg,LU)\n",
    "\n",
    "ts_all.append(ts_ETb)\n",
    "ts_all.append(ts_ETg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read GRACE dS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_dS_y=pd.read_csv(r\"D:\\FAO\\WA_Sheet1\\Main\\Jordan\\data\\GRACE\\dS_y_cmi.csv\",sep=';',index_col=0)\n",
    "# df_dS_y=pd.read_csv(r\"D:\\FAO\\WA_Sheet1\\Main\\Jordan\\data\\GRACE\\dS_y_Yarmouk_cmi.csv\",sep=';',index_col=0)\n",
    "# df_dS_y=pd.read_csv(r\"D:\\FAO\\WA_Sheet1\\Main\\Jordan\\data\\GRACE\\dS_y_Zarqa_cmi.csv\",sep=';',index_col=0)\n",
    "Area_skm=np.nansum(area)\n",
    "df_grace_ds=df_dS_y*Area_skm\n",
    "#df_grace_ds.index=[y.year for y in df_grace_ds.index]\n",
    "df_grace_ds=df_grace_ds.rename(columns={'dS [mm]':'GRACE_dS'})\n",
    "df_grace_ds=df_grace_ds.rename(columns={'Equivalent Water Height [mm]':'GRACE_dS'})\n",
    "ts_all.append(df_grace_ds)\n",
    "for i in range(len(ts_all)):\n",
    "    if i ==0:\n",
    "        df=ts_all[i]        \n",
    "    else:\n",
    "        df=pd.merge(df,ts_all[i],left_index=True,right_index=True,how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Qout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "df_q_out=pd.read_csv(r\"D:\\FAO\\WA_Sheet1\\Main\\Jordan\\data\\Q_NWC.csv\",sep=';',index_col=0)\n",
    "df_q_out.index=[datetime.datetime.strptime(y,'%d/%m/%Y') for y in df_q_out.index]\n",
    "\n",
    "df_q_out_y=df_q_out.resample('A-{0}'.format(end_month)).sum() #mean()\n",
    "df_q_out_y.index=[y.year for y in df_q_out_y.index]\n",
    "\n",
    "df=pd.merge(df,df_q_out_y,left_index=True,right_index=True,how='outer')\n",
    "\n",
    "df['dS']=(df['Precipitation']-df['Actual Evapotranspiration']-df['Q_NWC'])\n",
    "df['Error']=df['dS']-df['GRACE_dS']\n",
    "\n",
    "plt.plot(df['Error'],label='error')\n",
    "plt.plot(df['Q_NWC'],label='nwc')\n",
    "plt.plot(df['GRACE_dS'],label='grace')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Read Qout subbasins\n",
    "import datetime\n",
    "df_q_out=pd.read_csv(r\"D:\\FAO\\Jordan\\Obs_data\\Flows\\ADASIYIA.csv\",sep=';',index_col=0,skiprows=1,usecols=[0,4])\n",
    "df_q_out.index=[datetime.datetime.strptime(y,'%d/%m/%Y %H:%M') for y in df_q_out.index]\n",
    "##\n",
    "df_q_out=pd.read_csv(r\"D:\\FAO\\Jordan\\Obs_data\\Flows\\NEWJERASHBRIDGE.csv\",sep=';',index_col=0,skiprows=1,usecols=[0,4])\n",
    "df_q_out.index=[datetime.datetime.strptime(y,'%Y-%m-%d %H:%M:%S') for y in df_q_out.index]\n",
    "df_q_out=df_q_out.rename(columns={'data':'Qout'})\n",
    "\n",
    "df_q_out=df_q_out.replace(-9999,np.nan)\n",
    "df_q_out_y=df_q_out.resample('A-{0}'.format(end_month)).sum() #mean()\n",
    "df_q_out_y.index=[y.year for y in df_q_out_y.index]\n",
    "df_q_out_y=df_q_out_y.Qout*1000000 #from km3 to TCM\n",
    "df=pd.merge(df,df_q_out_y,left_index=True,right_index=True,how='outer')\n",
    "df=df.dropna()\n",
    "df['dS']=(df['Precipitation']-df['Actual Evapotranspiration']-df['Qout'])\n",
    "df['Error']=df['dS']-df['GRACE_dS']\n",
    "\n",
    "plt.plot(df['Error'],label='error')\n",
    "plt.plot(df['Qout'],label='Qout')\n",
    "plt.plot(df['GRACE_dS'],label='grace')\n",
    "plt.legend()\n",
    "df.to_csv(r'D:\\FAO\\WA_Sheet1\\Main\\Jordan\\subbasins\\df_Zarqa.csv',sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 8. Sheet 1 generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write fluxes volume table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    "# df.to_csv(r\"D:\\FAO\\WA_Sheet1\\Main\\Jordan\\df_all_191029.csv\",sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read fluxes volume table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r\"D:\\FAO\\WA_Sheet1\\Main\\Jordan\\df_all_191029.csv\",sep=';',index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Create Yearly Sheet 1 csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df/100000 #convert from TCM to 0.1 BCM\n",
    "csv_template=r\"D:\\FAO\\WA_Sheet1\\WAPORWA\\WA\\csv\\Sample_sheet1.csv\"\n",
    "csv_folder=os.path.join(Basin['Dir'],'data','csv')\n",
    "if not os.path.exists(csv_folder):\n",
    "    os.makedirs(csv_folder)\n",
    "df_template=pd.read_csv(csv_template,sep=\";\")\n",
    "csv_fhs=[]\n",
    "for year in df.index[1:-1]:\n",
    "    df_year=df_template.copy()\n",
    "    df_year['VALUE']=0.0\n",
    "    df_year.loc[0,'VALUE']=df.loc[year,'Precipitation']\n",
    "#    if df.loc[year,'dS']>0:\n",
    "#        df_year.loc[11,'VALUE']=df.loc[year,'dS']\n",
    "#    else:\n",
    "#        df_year.loc[10,'VALUE']=abs(df.loc[year,'dS'])\n",
    "    df_year.loc[10,'VALUE']=-df.loc[year,'dS']\n",
    "    df_year.loc[12,'VALUE']=df.loc[year,'Rainfall_ET_M-Protected Landuse']\n",
    "    df_year.loc[13,'VALUE']=df.loc[year,'Rainfall_ET_M-Utilized Landuse']\n",
    "    df_year.loc[14,'VALUE']=df.loc[year,'Rainfall_ET_M-Modified Landuse']\n",
    "    df_year.loc[15,'VALUE']=df.loc[year,'Rainfall_ET_M-Managed Water Use']\n",
    "    df_year.loc[16,'VALUE']=df.loc[year,'Increamental_ET_M-Protected Landuse']\n",
    "    df_year.loc[17,'VALUE']=df.loc[year,'Increamental_ET_M-Utilized Landuse']\n",
    "    df_year.loc[18,'VALUE']=df.loc[year,'Increamental_ET_M-Modified Landuse']\n",
    "    df_year.loc[19,'VALUE']=df.loc[year,'Increamental_ET_M-Managed Water Use']\n",
    "    df_year.loc[20,'VALUE']=df.loc[year,'Increamental_ET_M-Managed Water Use']\n",
    "    df_year.loc[21,'VALUE']=df.loc[year,'Increamental_ET_M']-df.loc[year,'Increamental_ET_M-Managed Water Use']\n",
    "    df_year.loc[26,'VALUE']=df.loc[year,'Q_NWC']\n",
    "    outcsv=os.path.join(csv_folder,'Sheet1_{0}.csv'.format(int(year)))\n",
    "    df_year.to_csv(outcsv,sep=\";\",index=False)\n",
    "    csv_fhs.append(outcsv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Create yearly Sheet 1 Png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% create sheet 1 png\n",
    "csv_folder=os.path.join(Basin['Dir'],'data','csv')\n",
    "csv_fhs=glob.glob(os.path.join(csv_folder,'Sheet1_*.csv'))\n",
    "svg_template=r\"D:\\FAO\\WA_Sheet1\\WAPORWA\\WA\\svg\\sheet_1_v2_0.1Bm3yr-1.svg\"\n",
    "png_folder=os.path.join(Basin['Dir'],'data','sheet1')\n",
    "if not os.path.exists(png_folder):\n",
    "    os.makedirs(png_folder)\n",
    "from WA.sheet1 import create_sheet1\n",
    "for csv in csv_fhs:\n",
    "    period=os.path.basename(csv).split('_')[1].split('.')[0] \n",
    "    output=os.path.join(png_folder,os.path.basename(csv).replace('csv','png'))\n",
    "    create_sheet1('Jordan', period, '0.1 BCM/year', csv, output, template=svg_template)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Create Mean annual sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Average sheet 1\n",
    "def average_sheet(sheet_folder,sheet_number,period):    \n",
    "    \"\"\"\n",
    "    Calculate average of yearly WA+ sheets in csv format\n",
    "    Applicable for Sheet2, 3a, 4, 5, 6\n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    sheet_folder: str\n",
    "        folder of all sheet csv\n",
    "    sheet_number: int\n",
    "        number from 1 to 6\n",
    "    period: list\n",
    "        period to calculate average [yyyy,yyyy]\n",
    "    unit: str\n",
    "        unit of values in average csv file (Mm3 or km3) default is Mm3\n",
    "        \n",
    "    Returns\n",
    "    -------------\n",
    "    outname: str\n",
    "        path to average csv\n",
    "    \"\"\"\n",
    "    \n",
    "    sheet_indexcol = {1: (0,1,2),\n",
    "                   2: (0,1),\n",
    "                   3: (0,1,2,3,4),\n",
    "                   4: (0),\n",
    "                   5: (0,1,3),\n",
    "                   6: (0,1)}\n",
    "    year_ls = pd.date_range(start='1/1/{0}'.format(period[0]),end='12/31/{0}'.format(period[1]),freq='Y')\n",
    "    \n",
    "    df_avg=0  \n",
    "    for year in year_ls:\n",
    "        sheet_csv = glob.glob(os.path.join(sheet_folder,'*{0}.csv'.format(str(year.year))))[0]\n",
    "        df=pd.read_csv(sheet_csv,sep=';',index_col=sheet_indexcol[sheet_number],na_values='nan')\n",
    "        df_avg+=df\n",
    "    df_avg=df_avg/len(year_ls)\n",
    "    \n",
    "    outname=os.path.join(sheet_folder,'sheet{0}_{1}-{2}.csv'.format(sheet_number,period[0],period[1]))\n",
    "    df_avg.to_csv(outname,sep=';',na_rep='nan')\n",
    "    return outname\n",
    "\n",
    "csv=average_sheet(r'D:\\FAO\\WA_Sheet1\\Main\\Jordan\\data\\csv',1,[2010,2018])\n",
    "#%%\n",
    "csv=r\"D:\\FAO\\WA_Sheet1\\Main\\Jordan\\data\\csv\\sheet1_2010-2018.csv\"\n",
    "output=os.path.join(png_folder,os.path.basename(csv).replace('csv','png'))\n",
    "create_sheet1('Jordan', '2010-2018', '0.1 BCM/year', csv, output, template=svg_template)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
