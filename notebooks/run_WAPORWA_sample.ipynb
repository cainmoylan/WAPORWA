{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5uB3f82pISu"
   },
   "source": [
    "# I. Initialize Notebook\n",
    "\n",
    "This Notebook is used to run the WaPOR-based Rapid Water Accounting+ Sheet 1 analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zyhGDM4Vp_D5"
   },
   "source": [
    "## 1. Requirements \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kPe3cW9opKtZ"
   },
   "outputs": [],
   "source": [
    "!pip install WaporIHE\n",
    "!pip install gdal\n",
    "!pip install netcdf4\n",
    "!pip install pyshp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cqWlCYD9-Y2K"
   },
   "source": [
    "## 2. Import packages and functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BeWxP0YwRN2M"
   },
   "source": [
    "**From installed packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4VOU-W57Nlqr"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import calendar\n",
    "import datetime\n",
    "import WaporIHE\n",
    "import gdal\n",
    "import ogr\n",
    "import shapefile\n",
    "import geopy\n",
    "import tempfile\n",
    "import csv\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TU8XXahNRIWG"
   },
   "source": [
    "**From WA_Sheet1 GitHub repo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O9tx6-vADOS2"
   },
   "outputs": [],
   "source": [
    "%cd /content/\n",
    "if os.path.exists('/content/WA_Sheet1'):\n",
    "  shutil.rmtree('/content/WA_Sheet1') \n",
    "!git clone https://github.com/trngbich/WA_Sheet1.git\n",
    "import WA_Sheet1\n",
    "from WA_Sheet1.pickle_basin import *\n",
    "from WA_Sheet1.LCC_to_LUWA import *\n",
    "from WA_Sheet1 import GIS_functions as gis\n",
    "from WA_Sheet1.create_NC import make_netcdf,get_lats_lons\n",
    "from WA_Sheet1.model_SMBalance import run_SMBalance,open_nc,merge_yearly_nc\n",
    "from WA_Sheet1.average_by_LU import Total_perLU\n",
    "from WA_Sheet1 import grace_functions as gf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ru6NweoCp7DR"
   },
   "source": [
    "## 3. Load data from Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A_x4MeT36i8e"
   },
   "source": [
    "**First, mount to Google drive folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u076vhDgqNjx"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y71QH6AOqyJC"
   },
   "source": [
    "**Second, copy the files from drive folder to work space**\n",
    "\n",
    "**MUST** \"Add to My Drive\"\n",
    "this Drive folder -> Link: https://drive.google.com/drive/folders/1OUKQOIHmtIfXIt5ACGzMCcf73xsGlXMw?usp=sharing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qlV5dPSyq-JD"
   },
   "outputs": [],
   "source": [
    "path_to_drive_folder=r'/content/drive/My Drive/WA/WA_Sheet1/.' #Change the path to where you add the data folder\n",
    "!cp -a '$path_to_drive_folder' '/content/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bmFLXc5xZzIp"
   },
   "source": [
    "#II. Initilize Basin "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hseFzpv_6BLG"
   },
   "source": [
    "## 1. Set up working folders for basin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OLHliZM4Vo2A"
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Jul 23 12:36:04 2019\n",
    "\n",
    "@author: ntr002\n",
    "\"\"\"\n",
    "WORKING_DIR=os.getcwd()\n",
    "MAIN_DIR=os.path.join(WORKING_DIR,'Main')\n",
    "INPUT_DIR=os.path.join(WORKING_DIR,'Input')\n",
    "\n",
    "### Global Input Info\n",
    "GRaND_SHP=os.path.join(INPUT_DIR,'Global','GRanD','GRanD_reservoirs_v1_1.shp')\n",
    "WDPA_SHP=os.path.join(INPUT_DIR,'Global','WDPA','WDPA_CatIandII_17countries.shp')\n",
    "GRACE_data=os.path.join(INPUT_DIR,'Global','GRACE','GSFC.glb.200301_201607_v02.4-ICE6G')\n",
    "ThetaSat_TIF=os.path.join(INPUT_DIR,'Global','HiHydroSoils','thetasat_topsoil.tif')\n",
    "\n",
    "### Basin Info\n",
    "BASIN_NAME='Litani'\n",
    "start_date='2009-01-01'\n",
    "end_date='2018-12-31'\n",
    "end_month='AUG'\n",
    "\n",
    "Qout_csv=r'/content/Input/Litani/Qoutlet_litani_QASMIYE_SeaMouth.csv'\n",
    "Qibt_csv=r'/content/Input/Litani/Qibt_litani_MarkabaTunnel.csv'\n",
    "BASIN_SHP=os.path.join(INPUT_DIR,'Global','Basins','{0}.shp'.format(BASIN_NAME))\n",
    "unreserv_SHP=None #to adjust Basin reservoir map\n",
    "makereserv_SHP=None #to adjust Basin reservoir map\n",
    "GRACEMonthly_CSV=os.path.join(INPUT_DIR,'Global','GRACE','{0}_GRACE.csv'.format(BASIN_NAME))\n",
    "\n",
    "###Create Basin folders\n",
    "INPUT_FOLDER=os.path.join(INPUT_DIR,BASIN_NAME)\n",
    "MAIN_FOLDER=os.path.join(MAIN_DIR,BASIN_NAME)\n",
    "for folder in [INPUT_FOLDER,MAIN_FOLDER]:\n",
    "  if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zslKiEeLaxOz"
   },
   "source": [
    "## 2. Download WaPOR data to Working folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pn4KiQuY8t4Z"
   },
   "source": [
    "**Instead of downloading from WaPOR API, copy sample (only Litani) data from Drive**\n",
    "\n",
    "MUST \"Add to My Drive\" this Drive folder -> Link: https://drive.google.com/drive/folders/1Z3FkWQlUzoBFD6dlbkWKIjJ-pwb1w3yy?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W1-otaXZ7kfI"
   },
   "outputs": [],
   "source": [
    "path_to_drive_folder=r'/content/drive/My Drive/WA/WA_Sheet1_Input_Litani/.' #Change the path to where you add the data folder\n",
    "!cp -a '$path_to_drive_folder' '$INPUT_FOLDER'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F6Wnq3KEKxSw"
   },
   "source": [
    "**OR Download new dataset from WaPOR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tfMJWRvDahCB"
   },
   "outputs": [],
   "source": [
    "shape=shapefile.Reader(BASIN_SHP)\n",
    "xmin,ymin,xmax,ymax=shape.bbox\n",
    "\n",
    "Token=input('Your WaPOR API Token: ')\n",
    "\n",
    "# WaporIHE.PCP_daily(APIToken=Token,Dir=INPUT_FOLDER,Startdate='2009-01-01',\n",
    "#                   Enddate='2010-01-01',latlim=[ymin-0.25,ymax+0.25],\n",
    "#                   lonlim=[xmin-0.25,xmax+0.25],version=2,level=1)\n",
    "\n",
    "# WaporIHE.PCP_monthly(APIToken=Token,Dir=INPUT_FOLDER,Startdate='2009-01-01',\n",
    "#                   Enddate='2010-01-01',latlim=[ymin-0.25,ymax+0.25],\n",
    "#                   lonlim=[xmin-0.25,xmax+0.25],version=2,level=1)\n",
    "\n",
    "# WaporIHE.RET_monthly(APIToken=Token,Dir=INPUT_FOLDER,Startdate='2009-01-01',\n",
    "#                   Enddate='2010-01-01',latlim=[ymin-0.5,ymax+0.5],\n",
    "#                   lonlim=[xmin-0.5,xmax+0.5],version=2,level=1)\n",
    "\n",
    "# WaporIHE.AET_monthly(APIToken=Token,Dir=INPUT_FOLDER,Startdate='2009-01-01',\n",
    "#                   Enddate='2010-01-01',latlim=[ymin,ymax],\n",
    "#                   lonlim=[xmin,xmax],version=2,level=2)\n",
    "\n",
    "# WaporIHE.LCC_yearly(APIToken=Token,Dir=INPUT_FOLDER,Startdate='2009-01-01',\n",
    "#                   Enddate='2010-01-01',latlim=[ymin,ymax],\n",
    "#                   lonlim=[xmin,xmax],version=2,level=2)\n",
    "\n",
    "# WaporIHE.I_dekadal(APIToken=Token,Dir=INPUT_FOLDER,Startdate='2009-01-01',\n",
    "#                   Enddate='2010-01-01',latlim=[ymin,ymax],\n",
    "#                   lonlim=[xmin,xmax],version=2,level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L-iB2XfG91Rc"
   },
   "source": [
    "## 3. Create metadata dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7RL7Q8tAEEyq"
   },
   "outputs": [],
   "source": [
    "BASIN={'Name':BASIN_NAME,\n",
    "       'time_range':[start_date,end_date],\n",
    "       'end_month':end_month,\n",
    "       'Dir':MAIN_FOLDER,       \n",
    "        'geo_data':{\n",
    "                   'basin':BASIN_SHP,\n",
    "                   'unreserv':unreserv_SHP, \n",
    "                   'makereserv':makereserv_SHP,\n",
    "                      },\n",
    "        'global_data':{                     \n",
    "                     'grand':GRaND_SHP,\n",
    "                     'wdpa':WDPA_SHP,\n",
    "                     'grace':GRACE_data\n",
    "                     },\n",
    "        'input_data':{\n",
    "                      'yearly':{\n",
    "                              'lcc':[r\"\"+os.path.join(INPUT_FOLDER,'L2_LCC_A'),\n",
    "                                     '-','Landcover Class'],\n",
    "                              },\n",
    "                      'monthly':{\n",
    "                              'p':[r\"\"+os.path.join(INPUT_FOLDER,'L1_PCP_M'),\n",
    "                                   'mm/month','Precipitation'],\n",
    "                              'et':[r\"\"+os.path.join(INPUT_FOLDER,'L2_AETI_M'),\n",
    "                                    'mm/month','Actual Evapotranspiration'],\n",
    "                              'ret':[r\"\"+os.path.join(INPUT_FOLDER,'L1_RET_M'),\n",
    "                                    'mm/month','Reference Evapotranspiration'],   \n",
    "                              'i': [r\"\"+os.path.join(INPUT_FOLDER,'L2_I_M'),\n",
    "                                    'mm/month','Interception'],                                                                   \n",
    "                              },  \n",
    "                    'stat':{                                                          \n",
    "                            'area':None,                 \n",
    "                            'thetasat':ThetaSat_TIF\n",
    "                            }                                                   \n",
    "                              },\n",
    "        'input_ts':{\n",
    "                'Qout':Qout_csv,\n",
    "                'Qibt':Qibt_csv\n",
    "                    },\n",
    "        'main_data':{\n",
    "                'yearly':{},\n",
    "                     'monthly':{},\n",
    "                     'stat':{},\n",
    "                     }                \n",
    "                }\n",
    "\n",
    "pickle_out(BASIN)\n",
    "BASIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "alVzpwPx29ZM"
   },
   "outputs": [],
   "source": [
    "BASIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jNbShxhmvD15"
   },
   "source": [
    "# III. Compute intermediate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "81MjGtZRwZF7"
   },
   "source": [
    "## 1. Calculate monthly Interception from L2_I_D\n",
    "**If L2_I_M is not available via API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AW2Rk9b_81Ca"
   },
   "outputs": [],
   "source": [
    "#Get list of dekadal rasters\n",
    "Dekadal_I_folder=os.path.join(INPUT_FOLDER,'L2_I_D')\n",
    "input_fhs=glob.glob(os.path.join(Dekadal_I_folder,'*.tif'))\n",
    "\n",
    "#Get month dates\n",
    "start_date=BASIN['time_range'][0]\n",
    "end_date=BASIN['time_range'][1]\n",
    "month_dates=pd.date_range(start_date,end_date,freq='M')\n",
    "\n",
    "#Get df avail\n",
    "ds_code='L2_I_D'\n",
    "WaporIHE.download.API.setAPIToken(Token)\n",
    "df_avail=WaporIHE.download.API.getAvailData(ds_code,time_range='{0},{1}'.format(start_date,end_date),version=2,level=2)\n",
    "\n",
    "output_folder=os.path.join(INPUT_FOLDER,'L2_I_M')\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "driver, NDV, xsize, ysize, GeoT, Projection = gis.GetGeoInfo(input_fhs[0])\n",
    "for date in month_dates:\n",
    "    month_fhs=[]\n",
    "    for in_fh in input_fhs:\n",
    "        raster_id=os.path.split(in_fh)[-1].split('.tif')[0][-9:]\n",
    "        raster_info=df_avail.loc[df_avail['raster_id']==raster_id]\n",
    "        timestr=raster_info['DEKAD'].iloc[0]\n",
    "        year=int(timestr[0:4])\n",
    "        month=int(timestr[5:7])\n",
    "        if (year==date.year)&(month==date.month):\n",
    "            month_fhs.append(in_fh)\n",
    "    SumArray=np.zeros((ysize,xsize),dtype=np.float32)\n",
    "    for fh in month_fhs:\n",
    "        Array=gis.OpenAsArray(fh,nan_values=True)\n",
    "        SumArray+=Array\n",
    "    out_fh=os.path.join(output_folder,'L2_I_{:04d}{:02d}.tif'.format(date.year,date.month))    \n",
    "    gis.CreateGeoTiff(out_fh, SumArray, driver, NDV, xsize, ysize, GeoT, Projection)\n",
    "\n",
    "BASIN['input_data']['monthly']['i']=[r\"\"+os.path.join(INPUT_FOLDER,'L2_I_M'),'mm/month','Interception']\n",
    "pickle_out(BASIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lf_jnlTvwgGa"
   },
   "source": [
    "##2. Calculate number of rainy days per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7xnXW-CvRjuL"
   },
   "outputs": [],
   "source": [
    "Data_Path_P=os.path.join(INPUT_FOLDER,'L1_PCP_E')\n",
    "\n",
    "Startdate=BASIN['time_range'][0]\n",
    "Enddate=BASIN['time_range'][1]\n",
    "\n",
    "Data_Path_RD = os.path.join(BASIN['Dir'],'data', 'Rainy_Days')\n",
    "if not os.path.exists(Data_Path_RD):\n",
    "    os.makedirs(Data_Path_RD)\n",
    "\n",
    "Dates = pd.date_range(Startdate, Enddate, freq ='MS')\n",
    "os.chdir(Data_Path_P)\n",
    "\n",
    "for Date in Dates:\n",
    "    # Define the year and month and amount of days in month\n",
    "    year = Date.year\n",
    "    month = Date.month\n",
    "    daysinmonth = calendar.monthrange(year, month)[1]\n",
    "\n",
    "    # Set the third (time) dimension of array starting at 0\n",
    "    i = 0\n",
    "\n",
    "    # Find all files of that month\n",
    "    files = glob.glob('*daily_%d.%02d.*.tif' %(year, month))\n",
    "\n",
    "    # Check if the amount of files corresponds with the amount of days in month\n",
    "    if len(files) is not daysinmonth:\n",
    "        print('ERROR: Not all Rainfall days for month %d and year %d are downloaded'  %(month, year))\n",
    "\n",
    "    # Loop over the days and store data in raster\n",
    "    for File in files:\n",
    "        dir_file = os.path.join(Data_Path_P, File)\n",
    "\n",
    "        # Get array information and create empty numpy array for daily rainfall when looping the first file\n",
    "        if File == files[0]:\n",
    "\n",
    "            # Open geolocation info and define projection\n",
    "            driver, NDV, xsize, ysize, GeoT, Projection = gis.GetGeoInfo(dir_file)\n",
    "\n",
    "            # Create empty array for the whole month\n",
    "            P_Daily = np.zeros([daysinmonth,ysize, xsize],dtype=np.float32)\n",
    "\n",
    "        # Open data and put the data in 3D array\n",
    "        Data = gis.OpenAsArray(dir_file,nan_values=True)\n",
    "\n",
    "        # Remove the weird numbers\n",
    "        Data[Data<0] = 0\n",
    "\n",
    "        # Add the precipitation to the monthly cube\n",
    "        P_Daily[i, :, :] = Data\n",
    "        i += 1\n",
    "\n",
    "    # Define a rainy day\n",
    "    P_Daily[P_Daily > 0.201] = 1\n",
    "    P_Daily[P_Daily != 1] = 0\n",
    "\n",
    "    # Sum the amount of rainy days\n",
    "    RD_one_month = np.nansum(P_Daily,0)\n",
    "\n",
    "    # Define output name\n",
    "    Outname = os.path.join(Data_Path_RD, 'Rainy_Days_NumOfDays_monthly_%d.%02d.01.tif' %(year, month))\n",
    "\n",
    "    # Save tiff file\n",
    "    gis.CreateGeoTiff(Outname, RD_one_month, driver, NDV, xsize, ysize, GeoT, Projection)\n",
    "    \n",
    "BASIN['input_data']['monthly']['nRD']=[Data_Path_RD,'days/month','Number of rainy days']\n",
    "pickle_out(BASIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xl0egDGrwmjJ"
   },
   "source": [
    "##3. Reclassify WaPOR LCC to LUWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fAmCJgRWXUur"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Thu Aug  8 13:42:44 2019\n",
    "\n",
    "@author: ntr002\n",
    "WAPOR LCC to LUWA\n",
    "\n",
    "Note: start Jupyter notebook with \"conda activate env\"\n",
    "\"\"\"\n",
    "### Get all yearly WaPOR LCC maps\n",
    "LCC_fhs=glob.glob(os.path.join(BASIN['input_data']['yearly']['lcc'][0],'*.tif'))\n",
    "WaPOR_LCC=LCC_fhs[0] #template\n",
    "\n",
    "### Create Reservoir raster map for basin\n",
    "Dir_out=os.path.join(BASIN['Dir'],'data','Reservoir')\n",
    "if not os.path.exists(Dir_out):\n",
    "    os.makedirs(Dir_out)\n",
    "Basin_Reservoir_tif=os.path.join(Dir_out,'Reservoir_basin.tif')\n",
    "Global_GRaND_Reservoir=BASIN['global_data']['grand']\n",
    "\n",
    "# adjust reservoir\n",
    "Resrv_to_Lake=BASIN['geo_data']['unreserv']\n",
    "Lake_to_Reserv=BASIN['geo_data']['makereserv']  \n",
    "if (Resrv_to_Lake!=None)&(Lake_to_Reserv!=None): #require 2 shapefiles to unreservoir and makereservoir           \n",
    "    Adjust_GRaND_reservoir(Basin_Reservoir_tif,WaPOR_LCC,\n",
    "                           Global_GRaND_Reservoir,Resrv_to_Lake,Lake_to_Reserv)    \n",
    "else:                \n",
    "    Rasterize_shapefile(Global_GRaND_Reservoir,WaPOR_LCC,Basin_Reservoir_tif)\n",
    "    \n",
    "BASIN['geo_data']['reservoir']=Basin_Reservoir_tif\n",
    "\n",
    "### Create Protected Area raster map for basin\n",
    "Dir_out=os.path.join(BASIN['Dir'],'data','Protected')\n",
    "if not os.path.exists(Dir_out):\n",
    "    os.makedirs(Dir_out)\n",
    "ProtectedArea_tif=os.path.join(Dir_out,'ProtectedArea_basin.tif')\n",
    "WDPA=BASIN['global_data']['wdpa']\n",
    "Rasterize_shapefile(WDPA,WaPOR_LCC,ProtectedArea_tif)\n",
    "\n",
    "### Reclassify LCC to LUWA       \n",
    "Dir_out=os.path.join(BASIN['Dir'],'data','luwa')\n",
    "if not os.path.exists(Dir_out):\n",
    "    os.makedirs(Dir_out)\n",
    "for fh in LCC_fhs:\n",
    "    Reclass_LCC_to_LUWA(fh,Dir_out,ProtectedArea_tif,Basin_Reservoir_tif)\n",
    "\n",
    "# Add LUWA folder to basin metadata\n",
    "BASIN['input_data']['yearly']['lu']=[Dir_out,'-','LUWA Categories']\n",
    "pickle_out(BASIN)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A52XvX2xw06H"
   },
   "source": [
    "# IV. Create netCDF for each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1z4f4iAySWVp"
   },
   "outputs": [],
   "source": [
    "BASIN['input_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qy3VofaJLfrH"
   },
   "outputs": [],
   "source": [
    "#Get inputs for create_NC\n",
    "cutline=BASIN['geo_data']['basin']\n",
    "Dir_out=os.path.join(BASIN['Dir'],'data','nc')\n",
    "if not os.path.exists(Dir_out):\n",
    "    os.makedirs(Dir_out)    \n",
    "template=glob.glob(os.path.join(BASIN['input_data']['monthly']['p'][0],'*.tif'))[0]          \n",
    "    \n",
    "## yearly maps\n",
    "# for key in ['p','et']:#BASIN['input_data']['yearly']:             \n",
    "#     if BASIN['input_data']['yearly'][key] is not None:\n",
    "#         nc_fn=os.path.join(Dir_out,key+'_yearly.nc')        \n",
    "#         dataset={key:[BASIN['input_data']['yearly'][key][0],\n",
    "#                        ('time','latitude', 'longitude'), \n",
    "#                        {'units': BASIN['input_data']['yearly'][key][1],                                 \n",
    "#                         'quantity':BASIN['input_data']['yearly'][key][2],\n",
    "#                         'source': 'WaPOR', 'period':'year'}]}\n",
    "#         succes=make_netcdf(nc_fn,BASIN['Name'],dataset,template,cutline,step='year')\n",
    "#         if succes:\n",
    "#             BASIN['main_data']['yearly'][key]=nc_fn\n",
    "#             print('Finished {0}_yearly.nc'.format(key))\n",
    "            \n",
    "### monthly maps  \n",
    "for key in ['p','et']:#BASIN['input_data']['monthly']:           \n",
    "    if BASIN['input_data']['monthly'][key] is not None:\n",
    "        nc_fn=os.path.join(Dir_out,key+'_monthly.nc')                \n",
    "        dataset={key:[BASIN['input_data']['monthly'][key][0],\n",
    "                       ('time','latitude', 'longitude'), \n",
    "                       {'units': BASIN['input_data']['monthly'][key][1],                                 \n",
    "                        'quantity':BASIN['input_data']['monthly'][key][2],\n",
    "                        'source': 'WaPOR', 'period':'month'}]}\n",
    "        succes=make_netcdf(nc_fn,BASIN['Name'],dataset,template,cutline,step='month')\n",
    "        if succes:\n",
    "            BASIN['main_data']['monthly'][key]=nc_fn\n",
    "            print('Finished {0}_monthly.nc'.format(key))\n",
    "  \n",
    "## static maps\n",
    "lats, lons, optionsProj, optionsClip =get_lats_lons(template,cutline)\n",
    "Dir_out=os.path.join(BASIN['Dir'],'data','stat')\n",
    "if not os.path.exists(Dir_out):\n",
    "    os.makedirs(Dir_out)\n",
    "for key in BASIN['input_data']['stat']:            \n",
    "    if BASIN['input_data']['stat'][key] is not None:  \n",
    "        basename= os.path.basename(BASIN['input_data']['stat'][key])\n",
    "        outfn=os.path.join(Dir_out,basename)  \n",
    "        temp_file = tempfile.NamedTemporaryFile(suffix='.tif').name               \n",
    "        gdal.Warp(temp_file, BASIN['input_data']['stat'][key], options = optionsProj)\n",
    "        gdal.Warp(outfn, temp_file, options = optionsClip)\n",
    "        os.remove(temp_file)\n",
    "        BASIN['main_data']['stat'][key]=outfn \n",
    "        print('Finished {0}'.format(basename))\n",
    "\n",
    "pickle_out(BASIN)   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RUkWAnDTw8Wn"
   },
   "source": [
    "## Correct ET of water bodies by ET reference\n",
    "**If ETa of water bodies is underestimated**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uJAz6Ot4pvZY"
   },
   "outputs": [],
   "source": [
    "#%% replace Et of water bodies by ET reference (based on FAO's recommendation)\n",
    "#lu\n",
    "lcc,_=open_nc(BASIN['main_data']['yearly']['lcc'])\n",
    "#ET\n",
    "et,_ = open_nc(BASIN['main_data']['monthly']['et'])\n",
    "#RET\n",
    "ret,_= open_nc(BASIN['main_data']['monthly']['ret'])\n",
    "\n",
    "lcc_dict = {\n",
    "                20: 'Shrubland',\n",
    "                30: 'Grassland', \n",
    "                41: 'Cropland, rainfed',\n",
    "                42: 'Cropland, irrigated or under water management', \n",
    "                43: 'Cropland, fallow', \n",
    "                50: 'Built-up', \n",
    "                60: 'Bare / sparse vegetation', \n",
    "                70: 'Permament snow / ice', \n",
    "                80: 'Water bodies', \n",
    "                81: 'Temporary water bodies', \n",
    "                90: 'Shrub or herbaceous cover, flooded', \n",
    "                111: 'Tree cover: closed, evergreen needle-leaved', \n",
    "                112: 'Tree cover: closed, evergreen broadleaved', \n",
    "                114: 'Tree cover: closed, deciduous broadleaved', \n",
    "                115: 'Tree cover: closed, mixed type', \n",
    "                116: 'Tree cover: closed, unknown type', \n",
    "                121: 'Tree cover: open, evergreen needle-leaved', \n",
    "                122: 'Tree cover: open, evergreen broadleaved', \n",
    "                123: 'Tree cover: open, deciduous needle-leaved', \n",
    "                124: 'Tree cover: open, deciduous broadleaved', \n",
    "                125: 'Tree cover: open, mixed type',\n",
    "                126: 'Tree cover: open, unknown type', \n",
    "                200: 'Sea water'\n",
    "          }\n",
    "##\n",
    "et_lcc_t = []\n",
    "\n",
    "for i in range(len(lcc.time)): \n",
    "    t1 = i*12\n",
    "    t2 = (i+1)*12\n",
    "    lcc_t = lcc.isel(time = i)\n",
    "    et_m_t = []\n",
    "    for t in range(t1,t2):\n",
    "        et_t = et.isel(time = t)  \n",
    "        ret_t = ret.isel(time = t)  \n",
    "        et_updated_t=et_t.where((lcc_t!=80)&(lcc_t!=200),ret_t)\n",
    "        #et_updated_t.assign_coords({'time': et_t.time})\n",
    "        et_updated_t = et_updated_t.assign_coords(time=et_t.time)\n",
    "        et_updated_t = et_updated_t.expand_dims('time')\n",
    "        et_m_t.append(et_updated_t)\n",
    "    et_m = xr.concat(et_m_t, dim = 'time')\n",
    "    et_lcc_t.append(et_m)\n",
    "et_modified= xr.concat(et_lcc_t, dim='time')  \n",
    "et_modified.name='Actual Evapotranspiration'\n",
    "et_modified=et_modified.chunk({\"latitude\":-1,\"longitude\":-1}).to_dataset()\n",
    "\n",
    "### save corrected ETa as netCDF\n",
    "nc_fn=r\"et_corrected_monthly.nc\"\n",
    "outfolder = os.path.join(BASIN['Dir'],'data','nc')\n",
    "nc_path=os.path.join(outfolder,nc_fn)\n",
    "et_modified.to_netcdf(nc_path,encoding={\"Actual Evapotranspiration\":{'zlib':True}})\n",
    "BASIN['main_data']['monthly']['et_corrected']=nc_path\n",
    "pickle_out(BASIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "26fOg5hCxENm"
   },
   "source": [
    "#V. Pixel-analysis: Run Soil Moisture Balance model to compute ET$_{incr}$ and ET$_{rain}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tBmvJGxrxfdF"
   },
   "source": [
    "## 1. Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aTDITQFFJ2JZ"
   },
   "outputs": [],
   "source": [
    "## input data\n",
    "smsat_file = BASIN['main_data']['stat']['thetasat']\n",
    "## read nc files    \n",
    "p_in = BASIN['main_data']['monthly']['p']\n",
    "e_in = BASIN['main_data']['monthly']['et_corrected'] #use corrected et \n",
    "#e_in = BASIN['main_data']['monthly']['et'] #use original et\n",
    "i_in = BASIN['main_data']['monthly']['i']\n",
    "rd_in = BASIN['main_data']['monthly']['nRD']\n",
    "lu_in = BASIN['main_data']['yearly']['lcc']\n",
    "\n",
    "## Run\n",
    "output_folder=os.path.join(BASIN['Dir'],'data','nc')\n",
    "(etrain_dir,etincr_dir)=run_SMBalance(output_folder,p_in,e_in,i_in,rd_in,lu_in,smsat_file,start_year=2009)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r3RQ31X1xiPs"
   },
   "source": [
    "##2. Merge resulted monthly ET$_{incr}$ and ET$_{rain}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AIU5nJ35maxj"
   },
   "outputs": [],
   "source": [
    "BASIN['main_data']['monthly']['etrain']=os.path.join(output_folder,'etrain_monthly.nc')\n",
    "BASIN['main_data']['monthly']['etincr']=os.path.join(output_folder,'etincr_monthly.nc')\n",
    "merge_yearly_nc(etrain_dir,BASIN['main_data']['monthly']['etrain'],varname='ET Rainfall')\n",
    "merge_yearly_nc(etincr_dir,BASIN['main_data']['monthly']['etincr'],varname='ET Incremental')\n",
    "pickle_out(BASIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WFCc02ApxpuN"
   },
   "source": [
    "# VI. Aggregate annual value by hydrological year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-W77Zd7gLTZL"
   },
   "outputs": [],
   "source": [
    "time='A-{0}'.format(BASIN['end_month'])\n",
    "for key in ['ret','et_corrected','i']:\n",
    "    nc=BASIN['main_data']['monthly'][key]\n",
    "    var,name=open_nc(nc)\n",
    "    var_y=var.resample(time=time).sum(dim='time',skipna=False)\n",
    "    outfolder=os.path.join(BASIN['Dir'],'data','nc')  \n",
    "    attrs={\"units\":\"mm/year\", \n",
    "           \"source\": \"Hydrolological year aggregation from {0}\".format(nc), \n",
    "           \"quantity\":name}\n",
    "    var_y.assign_attrs(attrs)\n",
    "    var_y.name = name\n",
    "    var_y_dts=var_y.chunk({\"latitude\":-1,\"longitude\":-1}).to_dataset()\n",
    "    nc_fn=\"{0}_hyearly.nc\".format(key)\n",
    "    nc_path=os.path.join(outfolder,nc_fn)\n",
    "    var_y_dts.to_netcdf(nc_path,encoding={name:{'zlib':True}})\n",
    "    BASIN['main_data']['yearly'][key]=nc_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fKyVOSBFUdKu"
   },
   "outputs": [],
   "source": [
    "time='A-{0}'.format(BASIN['end_month'])\n",
    "for key in ['p','et','etrain','etincr']:\n",
    "    nc=BASIN['main_data']['monthly'][key]\n",
    "    var,name=open_nc(nc)\n",
    "    var_y=var.resample(time=time).sum(dim='time',skipna=False)\n",
    "    outfolder=os.path.join(BASIN['Dir'],'data','nc')  \n",
    "    attrs={\"units\":\"mm/year\", \n",
    "           \"source\": \"Hydrolological year aggregation from {0}\".format(nc), \n",
    "           \"quantity\":name}\n",
    "    var_y.assign_attrs(attrs)\n",
    "    var_y.name = name\n",
    "    var_y_dts=var_y.chunk({\"latitude\":-1,\"longitude\":-1}).to_dataset()\n",
    "    nc_fn=\"{0}_hyearly.nc\".format(key)\n",
    "    nc_path=os.path.join(outfolder,nc_fn)\n",
    "    var_y_dts.to_netcdf(nc_path,encoding={name:{'zlib':True}})\n",
    "    BASIN['main_data']['yearly'][key]=nc_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V5Uld10Sx3ys"
   },
   "source": [
    "# V. Calculate volume fluxes table for Sheet 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NYjFuWid1423"
   },
   "source": [
    "##1. Select Region of Interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Q6b2vOblVw8"
   },
   "source": [
    "**Calculate area (km2) map**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zznydIiFkpWc"
   },
   "outputs": [],
   "source": [
    "# get template from LU map\n",
    "template=glob.glob(os.path.join(BASIN['input_data']['yearly']['lcc'][0],\"*.tif\"))[0]\n",
    "driver, NDV, xsize, ysize, GeoT, Projection = gis.GetGeoInfo(template)\n",
    "# calculate area per pixel\n",
    "area_map=gis.MapPixelAreakm(template)\n",
    "# save area map as tif\n",
    "BASIN['input_data']['stat']['area']=os.path.join(BASIN['Dir'],'data','stat','Area_km.tif')\n",
    "gis.CreateGeoTiff(BASIN['input_data']['stat']['area'],area_map,driver, NDV, xsize, ysize, GeoT, Projection)\n",
    "\n",
    "# create area mask\n",
    "shape=BASIN['geo_data']['basin']\n",
    "area=BASIN['input_data']['stat']['area']\n",
    "BASIN['main_data']['stat']['area']=os.path.join(BASIN['Dir'],'data','stat','Basin_Area_km.tif')\n",
    "gis.Clip_shapefile(area,shape,BASIN['main_data']['stat']['area'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W2q1xcB2lY5A"
   },
   "source": [
    "**Select Basin area**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b1jRZEEklbIC"
   },
   "outputs": [],
   "source": [
    "area_fh=BASIN['main_data']['stat']['area']\n",
    "# area_fh=Basin['main_data']['subbasin']\n",
    "area=gis.OpenAsArray(area_fh,nan_values=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dylmH2Bw2O6k"
   },
   "source": [
    "## 2. Calculate P, ET, ETincr, ETrain and per LUWA category\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u0hT5Yzh6MhD"
   },
   "source": [
    "**Calculate total P, ET, ETincr, ETrain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D2jFlpEAlnHb"
   },
   "outputs": [],
   "source": [
    "ts_all=[] #all timeseries\n",
    "for key in ['p','et','etrain','etincr']:\n",
    "    nc=BASIN['main_data']['yearly'][key]\n",
    "    dts=xr.open_dataset(nc)\n",
    "    var_name=list(dts.keys())[0]\n",
    "    var=dts[var_name].chunk({\"time\": 1, \"latitude\": 1000, \"longitude\": 1000}).ffill(\"time\")\n",
    "    Volume=var*area\n",
    "    ts=Volume.sum(dim=['latitude','longitude']).to_dataframe()\n",
    "    ts.index=[y.year for y in ts.index]\n",
    "    ts_all.append(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I42b0YJt6Py8"
   },
   "source": [
    "**Calculate ETincr, ETrain per LU category**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-XTO7uYumENL"
   },
   "outputs": [],
   "source": [
    "LU,_=open_nc(BASIN['main_data']['yearly']['lu'])\n",
    "ETrain,_=open_nc(BASIN['main_data']['yearly']['etrain'])\n",
    "ETincr,_=open_nc(BASIN['main_data']['yearly']['etincr'])\n",
    "\n",
    "### Different year date to year \n",
    "LU=LU.groupby('time.year').first(skipna=False)\n",
    "ETincr=area*ETincr.groupby('time.year').first(skipna=False)\n",
    "ETrain=area*ETrain.groupby('time.year').first(skipna=False)\n",
    "\n",
    "### average per LU\n",
    "ts_ETincr=Total_perLU(ETincr,LU)\n",
    "ts_ETrain=Total_perLU(ETrain,LU)\n",
    "\n",
    "ts_all.append(ts_ETincr)\n",
    "ts_all.append(ts_ETrain)\n",
    "\n",
    "BASIN['main_data']['ts_all']=ts_all\n",
    "pickle_out(BASIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A2A6ovOr2UoG"
   },
   "source": [
    "## 3. Calculate dS from GRACE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oXMmB1Vm31D_"
   },
   "source": [
    "**Calculate monthly dS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dE2huFoO0JDR"
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Jan 10 16:51:44 2018\n",
    "\n",
    "Need to download global GRACE GFSC product from\n",
    "https://ssed.gsfc.nasa.gov/grace/products.html\n",
    "GSFC.glb.200301_201607_v02.3b-ICE6G - ASCII\n",
    "\n",
    "@author: Claire Michailovsky \n",
    "\"\"\"\n",
    "\n",
    "BASIN_SHP=BASIN['geo_data']['basin']\n",
    "OUT_CSV=os.path.join(BASIN['Dir'],'grace','GRACE_monthly.csv')    \n",
    "os.makedirs(os.path.join(BASIN['Dir'],'grace'))\n",
    "MASCON_DATA_FOLDER = BASIN['global_data']['grace']\n",
    "\n",
    "BUFFER_SHP = OUT_CSV.split('.')[:-1][0]+'_buffer.shp'\n",
    "MASCON_SHP = OUT_CSV.split('.')[:-1][0]+'_mascons.shp'\n",
    "\n",
    "MASCON_INFO = os.path.join(MASCON_DATA_FOLDER, 'mascon.txt')\n",
    "MASCON_SOLUTION = os.path.join(MASCON_DATA_FOLDER, 'solution.txt')\n",
    "MASCON_DATES = os.path.join(MASCON_DATA_FOLDER, 'time.txt')\n",
    "\n",
    "BUFFER_DIST = .71\n",
    "gf.create_buffer(BASIN_SHP, BUFFER_SHP, BUFFER_DIST)\n",
    "df_info = pd.read_csv(MASCON_INFO, sep=r\"\\s+\", header=None, skiprows=14,engine='python')\n",
    "mascon_coords = zip(df_info[1], df_info[0])\n",
    "\n",
    "df_dates = pd.read_csv(MASCON_DATES, sep=r\"\\s+\", header=None, skiprows=13,engine='python')\n",
    "fract_dates = df_dates[2]\n",
    "mascon_dates = [str(gf.convert_partial_year(fdate)) for fdate in fract_dates]\n",
    "#?? Return null geometry sometimes??\n",
    "index_mascons_of_interest = gf.points_in_polygon(BUFFER_SHP, mascon_coords)\n",
    "#??\n",
    "data_lines = []\n",
    "with open(MASCON_SOLUTION) as fp:\n",
    "    for i, line in enumerate(fp):\n",
    "        if i in np.array(index_mascons_of_interest) + 7:\n",
    "            data_lines.append(np.array(line.rstrip('\\n').rstrip().split(' ')).astype(float))\n",
    "\n",
    "# Create .shp of mascon areas\n",
    "# Adapeted from bec's SortGRACE.py\n",
    "w = shapefile.Writer(MASCON_SHP, shapeType=shapefile.POLYGON)\n",
    "w.field('MASCON_ID', 'C', '40')\n",
    "\n",
    "for mascon_index in index_mascons_of_interest[0]:\n",
    "    ID = mascon_index+1\n",
    "    lon_center = df_info[1][mascon_index]\n",
    "    lat_center = df_info[0][mascon_index]\n",
    "    lon_span = df_info[3][mascon_index]\n",
    "    lat_span = df_info[2][mascon_index]\n",
    "    w.poly([\n",
    "            [[lon_center + .5 * lon_span, lat_center + .5 * lat_span],\n",
    "             [lon_center - .5 * lon_span, lat_center + .5 * lat_span],\n",
    "             [lon_center - .5 * lon_span, lat_center - .5 * lat_span],\n",
    "             [lon_center + .5 * lon_span, lat_center - .5 * lat_span],\n",
    "             [lon_center + .5 * lon_span, lat_center + .5 * lat_span]]\n",
    "            ])\n",
    "    w.record(ID,'Polygon')\n",
    "w.close()\n",
    "# Get weights from relative intersection area\n",
    "basin_poly = ogr.Open(BASIN_SHP)\n",
    "mascon_poly = ogr.Open(MASCON_SHP)\n",
    "\n",
    "basin_lyr = basin_poly.GetLayer()\n",
    "mascon_lyr = mascon_poly.GetLayer()\n",
    "for b_feature in basin_lyr:\n",
    "    ids = []\n",
    "    int_area = []\n",
    "    total_area = 0\n",
    "    for m_feature in mascon_lyr:\n",
    "        b_geom = b_feature.GetGeometryRef()\n",
    "        m_geom = m_feature.GetGeometryRef()\n",
    "        test = b_geom.Intersection(m_geom)\n",
    "        ids.append(m_feature.GetField(0))\n",
    "        int_area.append(test.GetArea())\n",
    "        total_area += test.GetArea()\n",
    "        \n",
    "weights = np.array(int_area)/total_area\n",
    "\n",
    "weighted_line = [data_lines[i] * weights[i] for i in range(len(data_lines))]\n",
    "weighted_average = np.sum(weighted_line, 0)\n",
    "\n",
    "with open(OUT_CSV, 'w',newline='') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter=',')\n",
    "    spamwriter.writerow(['date', 'Equivalent Water Height [mm]'])\n",
    "    for date, value in zip(mascon_dates, weighted_average):\n",
    "        spamwriter.writerow([date, value*10])\n",
    "\n",
    "BASIN['input_ts']['grace_monthly']=OUT_CSV\n",
    "pickle_out(BASIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CGeuni9p36xf"
   },
   "source": [
    "**Calculate yearly dS**\n",
    "\n",
    "$$\\Delta T\\overline WS _m ={TWSA _{m+1} - TWSA _{m-1}\\over 2}$$ (Biancamaria et al., 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nVduz0BY3wQK"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Based on Biancamaria et al. (2019)\n",
    "\n",
    "@author: ntr002\n",
    "'''\n",
    "###Read monthly csv\n",
    "ts_grace=pd.read_csv(BASIN['input_ts']['grace_monthly'],sep=',',index_col=0)\n",
    "\n",
    "###Interpolate daily data\n",
    "ts_day=pd.DataFrame(index=pd.date_range(start='2003-01-06',end='2016-07-14',freq='D'))\n",
    "ts_grace_daily=pd.merge(ts_day,ts_grace, left_index=True,right_index=True, how='outer')\n",
    "ts_grace_interpolate_daily=ts_grace_daily.interpolate()\n",
    "# ts_grace_interpolate_daily.plot(style='.-')\n",
    "\n",
    "###Calculate cumulative sum of dS\n",
    "ts_dS_daily=ts_grace_interpolate_daily.diff()\n",
    "# ts_dS_daily.plot()\n",
    "ts_cumsum_dS_daily=ts_dS_daily.cumsum()\n",
    "# ts_cumsum_dS_daily.plot()\n",
    "\n",
    "###Calculate monthly dS by second order central difference between cumsum dS\n",
    "ts_cumsum_dS_monthly_firstday=ts_cumsum_dS_daily.resample('MS').first()\n",
    "ts_cumsum_dS_monthly_firstday #take value of 1st day of every month\n",
    "ts_dS_monthly=ts_cumsum_dS_monthly_firstday.diff(2).shift(-1)/2\n",
    "# ts_dS_monthly\n",
    "\n",
    "###Calculate (hydrological) yearly dS\n",
    "ts_dS_yearly=ts_dS_monthly.resample('A-{0}'.format(end_month)).sum()\n",
    "ts_dS_yearly.index=ts_dS_yearly.index.year\n",
    "\n",
    "###Save results\n",
    "out_csv=os.path.join(BASIN['Dir'],'grace','GRACE_yearly.csv')\n",
    "ts_dS_yearly[6:-1].to_csv(out_csv,sep=';') #only from 2009 to 2015 since 2016 is not complete\n",
    "BASIN['input_ts']['grace_yearly']=out_csv\n",
    "pickle_out(BASIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ztT1U-qx9COV"
   },
   "source": [
    "**Convert dS to volume**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UESv35Vo9FZk"
   },
   "outputs": [],
   "source": [
    "df_dS_y=pd.read_csv(BASIN['input_ts']['grace_yearly'],sep=';',index_col=0)\n",
    "Area_skm=np.nansum(area)\n",
    "df_grace_ds=df_dS_y*Area_skm\n",
    "#df_grace_ds.index=[y.year for y in df_grace_ds.index]\n",
    "df_grace_ds=df_grace_ds.rename(columns={'Equivalent Water Height [mm]':'GRACE_dS'})\n",
    "ts_all.append(df_grace_ds)\n",
    "\n",
    "BASIN['main_data']['ts_all']=ts_all\n",
    "pickle_out(BASIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vwhlx7pq2jBD"
   },
   "source": [
    "## 4. Read Outflows Qout, Qibt (inter-basin transfer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UKD1tqjlCZ0U"
   },
   "source": [
    "**Read Qout**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yIQ9eE0b-c20"
   },
   "outputs": [],
   "source": [
    "if BASIN['input_ts']['Qout'] is None:\n",
    "  df_Qout_y=ts_all[0]*0\n",
    "  Qout=df_Qout_y.rename(colums={'Precipitation':'Qout'})\n",
    "else:\n",
    "  Qout_m=pd.read_csv(BASIN['input_ts']['Qout'],sep=';',index_col=0,skiprows=1)\n",
    "  Qout_m.index=[datetime.datetime.strptime(y,'%d/%m/%Y %H:%M') for y in Qout_m.index]\n",
    "  Qout_m=Qout_m.replace(-9999,np.nan)\n",
    "  Qout_y=Qout_m.resample('A-{0}'.format(end_month)).sum() #mean()\n",
    "  Qout_y.index=[y.year for y in Qout_y.index]\n",
    "  ##\n",
    "  df_Qout_y=Qout_y.where(Qout_y.days>=365) ##remove years with missing data\n",
    "  df_Qout_y=df_Qout_y['km3/month']*1000000 ##convert to TCM=km2*mm\n",
    "  df_Qout_y=df_Qout_y.dropna()\n",
    "  df_Qout_y.name='Qout'\n",
    "  Qout=df_Qout_y.to_frame()\n",
    "\n",
    "ts_all.append(Qout)\n",
    "BASIN['main_data']['ts_all']=ts_all\n",
    "pickle_out(BASIN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IW6LsPTwCdzR"
   },
   "source": [
    "**Read Qibt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nmFYiCZdCfs4"
   },
   "outputs": [],
   "source": [
    "if BASIN['input_ts']['Qibt'] is None:\n",
    "  df_Qibt_y=ts_all[0]*0\n",
    "  Qibt=df_Qibt_y.rename(colums={'Precipitation':'Qibt'})\n",
    "else:\n",
    "  Qibt_m=pd.read_csv(BASIN['input_ts']['Qibt'],sep=';',index_col=0,skiprows=1)\n",
    "  Qibt_m.index=[datetime.datetime.strptime(y,'%d/%m/%Y %H:%M') for y in Qibt_m.index]\n",
    "  Qibt_m=Qibt_m.replace(-9999,np.nan)\n",
    "  Qibt_y=Qibt_m.resample('A-{0}'.format(end_month)).sum() #mean()\n",
    "  Qibt_y.index=[y.year for y in Qibt_y.index]\n",
    "  ##\n",
    "  df_Qibt_y=Qibt_y.where(Qibt_y.days>=365) ##remove years with missing data\n",
    "  df_Qibt_y=df_Qibt_y['km3/month']*1000000 ##convert to TCM=km2*mm\n",
    "  df_Qibt_y=df_Qibt_y.dropna()\n",
    "  df_Qibt_y.name='Qibt'\n",
    "  Qibt=df_Qibt_y.to_frame()\n",
    "\n",
    "ts_all.append(Qibt)\n",
    "BASIN['main_data']['ts_all']=ts_all\n",
    "pickle_out(BASIN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y8QxpytdFmMf"
   },
   "source": [
    "##5. Estimate error and export volume fluxes results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BequCHeh9TAh"
   },
   "outputs": [],
   "source": [
    "for i in range(len(ts_all)):\n",
    "    if i ==0:\n",
    "        df=ts_all[i]        \n",
    "    else:\n",
    "        df=pd.merge(df,ts_all[i],left_index=True,right_index=True,how='outer')\n",
    "\n",
    "df['WB_dS']=df['Precipitation']-df['Actual Evapotranspiration']-df['Qout']-df['Qibt']\n",
    "df['dS_Error']=df['GRACE_dS']-df['WB_dS']\n",
    "df.to_csv(os.path.join(BASIN['Dir'],'df_all.csv'),sep=';')\n",
    "BASIN['main_data']['df_all']=os.path.join(BASIN['Dir'],'df_all.csv')\n",
    "pickle_out(BASIN)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R_G0lKyb2oO8"
   },
   "source": [
    "# VI. Generate Sheet 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5nlTIm7L2rnR"
   },
   "source": [
    "## 1. Create Yearly Sheet 1 csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ESF6Tg_SQaL"
   },
   "outputs": [],
   "source": [
    "#Get sheet1 csv template\n",
    "csv_template=os.path.join(WORKING_DIR,'WA_Sheet1','csv','Sample_sheet1.csv')\n",
    "\n",
    "# convert_unit=1000000 #from TCM to BCM\n",
    "convert_unit=1000 #from TCM to MCM\n",
    "\n",
    "df=pd.read_csv(BASIN['main_data']['df_all'],sep=';',index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8iv1Kb9gSDrs"
   },
   "outputs": [],
   "source": [
    "df=df/convert_unit\n",
    "csv_folder=os.path.join(BASIN['Dir'],'data','csv')\n",
    "if not os.path.exists(csv_folder):\n",
    "    os.makedirs(csv_folder)\n",
    "\n",
    "df_template=pd.read_csv(csv_template,sep=\";\")\n",
    "csv_fhs=[]\n",
    "for year in df.index[1:-1]:\n",
    "    df_year=df_template.copy()\n",
    "    df_year['VALUE']=0.0\n",
    "    df_year.loc[0,'VALUE']=df.loc[year,'Precipitation']\n",
    "#    if df.loc[year,'dS']>0:\n",
    "#        df_year.loc[11,'VALUE']=df.loc[year,'dS']\n",
    "#    else:\n",
    "#        df_year.loc[10,'VALUE']=abs(df.loc[year,'dS'])\n",
    "    df_year.loc[10,'VALUE']=-df.loc[year,'WB_dS']\n",
    "    df_year.loc[12,'VALUE']=df.loc[year,'etrain-PLU']\n",
    "    df_year.loc[13,'VALUE']=df.loc[year,'etrain-ULU']\n",
    "    df_year.loc[14,'VALUE']=df.loc[year,'etrain-MLU']\n",
    "    df_year.loc[15,'VALUE']=df.loc[year,'etrain-MWU']\n",
    "    df_year.loc[16,'VALUE']=df.loc[year,'etincr-PLU']\n",
    "    df_year.loc[17,'VALUE']=df.loc[year,'etincr-ULU']\n",
    "    df_year.loc[18,'VALUE']=df.loc[year,'etincr-MLU']\n",
    "    df_year.loc[19,'VALUE']=df.loc[year,'etincr-MWU']\n",
    "    df_year.loc[20,'VALUE']=df.loc[year,'etincr-MWU']\n",
    "    df_year.loc[21,'VALUE']=df.loc[year,'etincr']-df.loc[year,'etincr-MWU']\n",
    "    df_year.loc[26,'VALUE']=df.loc[year,'Qibt']\n",
    "    df_year.loc[22,'VALUE']=df.loc[year,'Qout']\n",
    "    outcsv=os.path.join(csv_folder,'Sheet1_{0}.csv'.format(int(year)))\n",
    "    df_year.to_csv(outcsv,sep=\";\",index=False)\n",
    "    csv_fhs.append(outcsv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AtIRDsjy2zAL"
   },
   "source": [
    "## 2. Calculate mean annual Sheet 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sCyIZ-GCXLo3"
   },
   "outputs": [],
   "source": [
    "#%% Average sheet 1\n",
    "def average_sheet(sheet_folder,sheet_number,period):    \n",
    "    \"\"\"\n",
    "    Calculate average of yearly WA+ sheets in csv format\n",
    "    Applicable for Sheet2, 3a, 4, 5, 6\n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    sheet_folder: str\n",
    "        folder of all sheet csv\n",
    "    sheet_number: int\n",
    "        number from 1 to 6\n",
    "    period: list\n",
    "        period to calculate average [yyyy,yyyy]\n",
    "    unit: str\n",
    "        unit of values in average csv file (Mm3 or km3) default is Mm3\n",
    "        \n",
    "    Returns\n",
    "    -------------\n",
    "    outname: str\n",
    "        path to average csv\n",
    "    \"\"\"\n",
    "    \n",
    "    sheet_indexcol = {1: (0,1,2),\n",
    "                   2: (0,1),\n",
    "                   3: (0,1,2,3,4),\n",
    "                   4: (0),\n",
    "                   5: (0,1,3),\n",
    "                   6: (0,1)}\n",
    "    year_ls = pd.date_range(start='1/1/{0}'.format(period[0]),end='12/31/{0}'.format(period[1]),freq='Y')\n",
    "    \n",
    "    df_avg=0  \n",
    "    for year in year_ls:\n",
    "        sheet_csv = glob.glob(os.path.join(sheet_folder,'*{0}.csv'.format(str(year.year))))[0]\n",
    "        df=pd.read_csv(sheet_csv,sep=';',index_col=sheet_indexcol[sheet_number],na_values='nan')\n",
    "        df_avg+=df\n",
    "    df_avg=df_avg/len(year_ls)\n",
    "    \n",
    "    outname=os.path.join(sheet_folder,'sheet{0}_{1}-{2}.csv'.format(sheet_number,period[0],period[1]))\n",
    "    df_avg.to_csv(outname,sep=';',na_rep='nan')\n",
    "    return outname\n",
    "\n",
    "csv=average_sheet(os.path.join(BASIN['Dir'],'data','csv'),1,[2010,2018])\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "run_WAPORWA.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
